<!DOCTYPE html><!--Cpj0gCM6TtTMEc44BRv8y--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/0484562807a97172-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/4c285fdca692ea22-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/6245472ced48d3be-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/7108afb8b1381ad1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/7db6c35d839a711c-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/8888a3826f4a3af4-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/9e82d62334b205f4-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/b957ea75a84b6ea7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/eafabf029ad39a43-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/32300a44191f1a85.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1862799ef07f1502.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/9941ba66c3b4d9a3.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/79bf3a80da995a41.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c72594012cddfabc.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-bb13c56857e83941.js"/><script src="/_next/static/chunks/4bd1b696-409494caf8c83275.js" async=""></script><script src="/_next/static/chunks/255-8db1c35057a14be6.js" async=""></script><script src="/_next/static/chunks/main-app-4ffd0bfd209b03a4.js" async=""></script><script src="/_next/static/chunks/619-28045d4483afb7ae.js" async=""></script><script src="/_next/static/chunks/app/layout-6f68f2f8099a5bef.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-9ec1460c9a74315b.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-HRQ77GT2C8" as="script"/><meta name="next-size-adjust" content=""/><title>Working on text classification? Use a task-specific LLM. | Tanaos Blog</title><meta name="keywords" content="task-specific LLM,AI,text classification,without training data,NLP,machine learning"/><link rel="canonical" href="undefined/blog/task-specific-llms/"/><meta property="og:title" content="Working on text classification? Use a task-specific LLM."/><meta property="og:url" content="undefined/blog/task-specific-llms/"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Working on text classification? Use a task-specific LLM."/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_9b9fd1"><div hidden=""><!--$--><!--/$--></div><div class="app"><div class="row Navbar_navbar__3CvTR m-0 false"><div class="m-0 p-0 Navbar_navbar-large-devices__rsA5K"><div class="col m-0 p-0 text-start"><span class="logo">Tanaos</span></div><div class="col m-0 p-0 text-end"><div class="Navigation_navigation__Eln2g"><a class="mt-4 mt-md-0 ms-md-5 false" href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Platform</a><a class="btn btn-white mt-4 mt-md-0 ms-md-4" href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">See on GitHub <i class="bi bi-github ms-2"></i></a></div></div></div><div class="Navbar_navbar-small-devices__w8Bl9 d-md-none m-0 p-0 d-flex align-items-center"><div class="col-10 m-0 p-0 text-start"><span class="logo">Tanaos</span></div><div class="col-2 m-0 p-0 text-end"><i class="bi bi-list Navbar_navbar-toggle-icon__TXjJe"></i></div></div><div class="d-md-none Navbar_navbar-collapse__09PzW text-start false"><div class="mt-4"><div class="Navigation_navigation__Eln2g"><a class="mt-4 mt-md-0 ms-md-5 false" href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Platform</a><a class="btn btn-white mt-4 mt-md-0 ms-md-4" href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">See on GitHub <i class="bi bi-github ms-2"></i></a></div></div></div></div><main class="content"><article class="BlogPage_article__fv9gc"><a class="BlogPage_back-link__G6xqk" href="/blog/"><i class="bi bi-arrow-left"></i> Back to blog</a><h1 class="mt-4">Working on text classification? Use a task-specific LLM.</h1><p class="BlogPage_subtitle__wqaGw">Here is why you should stop using general-purpose LLMs for narrow tasks like text classification.</p><p class="BlogPage_date__VyN7U">September 15, 2025</p><div class="TLDRBox_tldr-box__UF_9W"><h3 class="TLDRBox_tldr-title__YbjzN">TL;DR</h3><p>When working on narrow, well-defined text classification tasks, general-purpose LLMs are typically overkill,
overpriced and underperforming. Task-specific LLMs are smaller, cheaper (often free) to run and perform
better on these tasks. Our <a href="https://github.com/tanaos/artifex" rel="noreferrer" target="_blank">
Artifex Python library <i class="bi bi-box-arrow-up-right"></i>
</a> makes it easy to create and use task-specific LLMs
without the need for any training data, so you can reduce the number of paid API calls to third-party
LLM APIs.</p></div>
<p>General-purpose LLMs are trained to be versatile across as wide a range of tasks as possible. They
handle themselves well in scenarios extending from code generation to creative writing. While this is very quickly
turning them into an irreplaceable tool for many, it also means that they come with two main limitations:</p>
<ol>
<li><strong>Cost</strong>: as their very name suggests, LLMs are <em>extremely large</em>. This means that they are expensive to run, and
their use requires either specialized hardware, or access to a paid third-party API.</li>
<li><strong>Non-specificity</strong>: because they are trained to be general-purpose, LLMs are not particularly good at
tasks that require domain knowledge, or being trained on a specific dataset.</li>
</ol>
<p>In practice, this means that, when working on many narrow, well-defined tasks, general-purpose LLMs are often not
just <strong>overkill</strong>, but also <strong>underperforming</strong>.</p>
<h2>Text classification</h2>
<p>Text classification is a prime example of this. While their formulation and objective are straightforward, many
text classification tasks such as <strong>topic categorization</strong>, <strong>intent classification</strong> or <strong>safety filtering</strong>
require understanding of the specific domain they are applied to, which LLMs can only obtain through <strong>fine-tuning
on a relevant dataset</strong>.</p>
<p>Think, for instance, of a <strong>safety filtering</strong> task, where the goal is to flag text that contains unwanted content
as <em>unsafe</em>. Since the definition of <em>unwanted content</em> really depends on the user&#x27;s requirements, the only way
to get a general-purpose LLM to perform well on this task is through thorough and <strong>meticulous prompt engineering</strong>.
Even then, the results are often subpar, and the cost of running a general-purpose LLM for this relatively simple
task is non-trivial (especially at scale).</p>
<p>The same applies to a number of other text classification tasks, such as <strong>intent classification</strong>, where the LLM
needs to be provided with a list of possible intents and examples for each, <strong>slot filling</strong>, where the LLM needs
to be told what slots to fill and how, and so on.</p>
<h2>Task-specific LLMs</h2>
<p>Task-specific LLMs are smaller, more focused LLMs that are trained to perform very well on a specific task, and not
necessarily on anything else. Due to their smaller size, they are <strong>much cheaper to run</strong> than general-purpose LLMs,
and they can be deployed on more modest hardware.</p>
<p>More importantly, task-specific LLMs are <strong>trained on datasets that are relevant to the task at hand</strong>. This means
that they can leverage domain knowledge and perform much better than general-purpose LLMs on narrow tasks.
For instance, a task-specific LLM for <strong>safety filtering</strong> would be trained on a dataset of safe and unsafe
messages, as per the user&#x27;s definition of safety, and would learn to identify patterns and features that are
indicative of unwanted content.</p>
<h2>How Tanaos can help</h2>
<p>Our <a href="https://github.com/tanaos/artifex" rel="noreferrer" target="_blank">Artifex Python library</a> provides
a simple and efficient way to create and use small-sized, task-specific LLMs for text classification tasks.
Most importantly, Artifex allows you to create task-specific LLMs <strong>without the need for any training data</strong>.
You simply describe how the model should behave, and it will be trained on synthetic data generated
for that purpose.</p>
<p>The models created with Artifex are so lightweight that they can <strong>run locally or on small servers
without a GPU</strong>, offloading simple tasks and reducing reliance on third-party LLM APIs.</p>
<h2>Give it a try</h2>
<p>Artifex is free and open-source. Check it out on GitHub!</p>
<a class="btn btn-primary" href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">See Artifex on GitHub <i class="bi bi-github ms-2"></i></a></article><!--$--><!--/$--></main><div class="row Footer_footer__Ir1kR m-0 align-items-center"><div class="col-12 col-md-4 m-0 p-0 text-start">Copyright © <!-- -->2025<!-- --> Tanaos</div><div class="col-md-4 text-md-center d-none d-md-block">Tanaos</div><div class="col-12 col-md-4 text-md-end mt-4 mt-md-0 Footer_footer-links__9Skqm m-0 p-0"><a href="/">Home</a><a href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank">Platform</a></div></div></div><script src="/_next/static/chunks/webpack-bb13c56857e83941.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[95539,[\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"177\",\"static/chunks/app/layout-6f68f2f8099a5bef.js\"],\"Navbar\"]\n3:I[9766,[],\"\"]\n4:I[98924,[],\"\"]\n5:I[52619,[\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-9ec1460c9a74315b.js\"],\"\"]\n6:I[68332,[\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"177\",\"static/chunks/app/layout-6f68f2f8099a5bef.js\"],\"GoogleAnalytics\"]\n8:I[24431,[],\"OutletBoundary\"]\na:I[15278,[],\"AsyncMetadataOutlet\"]\nc:I[24431,[],\"ViewportBoundary\"]\ne:I[24431,[],\"MetadataBoundary\"]\nf:\"$Sreact.suspense\"\n11:I[57150,[],\"\"]\n:HL[\"/_next/static/media/0484562807a97172-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/4c285fdca692ea22-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/6245472ced48d3be-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/7108afb8b1381ad1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/7db6c35d839a711c-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/8888a3826f4a3af4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/9e82d62334b205f4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/b957ea75a84b6ea7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/eafabf029ad39a43-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/32300a44191f1a85.css\",\"style\"]\n:HL[\"/_next/static/css/1862799ef07f1502.css\",\"style\"]\n:HL[\"/_next/static/css/9941ba66c3b4d9a3.css\",\"style\"]\n:HL[\"/_next/static/css/79bf3a80da995a41.css\",\"style\"]\n:HL[\"/_next/static/css/c72594012cddfabc.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Cpj0gCM6TtTMEc44BRv8y\",\"p\":\"\",\"c\":[\"\",\"blog\",\"task-specific-llms\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"task-specific-llms\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/32300a44191f1a85.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1862799ef07f1502.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/9941ba66c3b4d9a3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/79bf3a80da995a41.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"body\",null,{\"className\":\"__className_9b9fd1\",\"children\":[\"$\",\"div\",null,{\"className\":\"app\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"content\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"div\",null,{\"className\":\"row Footer_footer__Ir1kR m-0 align-items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-12 col-md-4 m-0 p-0 text-start\",\"children\":[\"Copyright © \",2025,\" Tanaos\"]}],[\"$\",\"div\",null,{\"className\":\"col-md-4 text-md-center d-none d-md-block\",\"children\":\"Tanaos\"}],[\"$\",\"div\",null,{\"className\":\"col-12 col-md-4 text-md-end mt-4 mt-md-0 Footer_footer-links__9Skqm m-0 p-0\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"children\":\"Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.tanaos.com/artifex/\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"href\":\"https://platform.tanaos.com\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Platform\"}]]}]]}]]}]}],[\"$\",\"$L6\",null,{\"gaId\":\"G-HRQ77GT2C8\"}]]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"task-specific-llms\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c72594012cddfabc.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":[\"$L9\",[\"$\",\"$La\",null,{\"promise\":\"$@b\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Le\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$f\",null,{\"fallback\":null,\"children\":\"$L10\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"article\",null,{\"className\":\"BlogPage_article__fv9gc\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"className\":\"BlogPage_back-link__G6xqk\",\"children\":[[\"$\",\"i\",null,{\"className\":\"bi bi-arrow-left\"}],\" Back to blog\"]}],[\"$\",\"h1\",null,{\"className\":\"mt-4\",\"children\":\"Working on text classification? Use a task-specific LLM.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_subtitle__wqaGw\",\"children\":\"Here is why you should stop using general-purpose LLMs for narrow tasks like text classification.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_date__VyN7U\",\"children\":\"September 15, 2025\"}],[[\"$\",\"div\",null,{\"className\":\"TLDRBox_tldr-box__UF_9W\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"TLDRBox_tldr-title__YbjzN\",\"children\":\"TL;DR\"}],[\"$\",\"p\",null,{\"children\":[\"When working on narrow, well-defined text classification tasks, general-purpose LLMs are typically overkill,\\noverpriced and underperforming. Task-specific LLMs are smaller, cheaper (often free) to run and perform\\nbetter on these tasks. Our \",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":[\"\\nArtifex Python library \",[\"$\",\"i\",null,{\"className\":\"bi bi-box-arrow-up-right\"}],\"\\n\"]}],\" makes it easy to create and use task-specific LLMs\\nwithout the need for any training data, so you can reduce the number of paid API calls to third-party\\nLLM APIs.\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"General-purpose LLMs are trained to be versatile across as wide a range of tasks as possible. They\\nhandle themselves well in scenarios extending from code generation to creative writing. While this is very quickly\\nturning them into an irreplaceable tool for many, it also means that they come with two main limitations:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cost\"}],\": as their very name suggests, LLMs are \",[\"$\",\"em\",null,{\"children\":\"extremely large\"}],\". This means that they are expensive to run, and\\ntheir use requires either specialized hardware, or access to a paid third-party API.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Non-specificity\"}],\": because they are trained to be general-purpose, LLMs are not particularly good at\\ntasks that require domain knowledge, or being trained on a specific dataset.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In practice, this means that, when working on many narrow, well-defined tasks, general-purpose LLMs are often not\\njust \",[\"$\",\"strong\",null,{\"children\":\"overkill\"}],\", but also \",[\"$\",\"strong\",null,{\"children\":\"underperforming\"}],\".\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Text classification\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Text classification is a prime example of this. While their formulation and objective are straightforward, many\\ntext classification tasks such as \",[\"$\",\"strong\",null,{\"children\":\"topic categorization\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"intent classification\"}],\" or \",[\"$\",\"strong\",null,{\"children\":\"safety filtering\"}],\"\\nrequire understanding of the specific domain they are applied to, which LLMs can only obtain through \",[\"$\",\"strong\",null,{\"children\":\"fine-tuning\\non a relevant dataset\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Think, for instance, of a \",[\"$\",\"strong\",null,{\"children\":\"safety filtering\"}],\" task, where the goal is to flag text that contains unwanted content\\nas \",[\"$\",\"em\",null,{\"children\":\"unsafe\"}],\". Since the definition of \",[\"$\",\"em\",null,{\"children\":\"unwanted content\"}],\" really depends on the user's requirements, the only way\\nto get a general-purpose LLM to perform well on this task is through thorough and \",[\"$\",\"strong\",null,{\"children\":\"meticulous prompt engineering\"}],\".\\nEven then, the results are often subpar, and the cost of running a general-purpose LLM for this relatively simple\\ntask is non-trivial (especially at scale).\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The same applies to a number of other text classification tasks, such as \",\"$L12\",\", where the LLM\\nneeds to be provided with a list of possible intents and examples for each, \",\"$L13\",\", where the LLM needs\\nto be told what slots to fill and how, and so on.\"]}],\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\"]]}]\n"])</script><script>self.__next_f.push([1,"12:[\"$\",\"strong\",null,{\"children\":\"intent classification\"}]\n13:[\"$\",\"strong\",null,{\"children\":\"slot filling\"}]\n14:[\"$\",\"h2\",null,{\"children\":\"Task-specific LLMs\"}]\n15:[\"$\",\"p\",null,{\"children\":[\"Task-specific LLMs are smaller, more focused LLMs that are trained to perform very well on a specific task, and not\\nnecessarily on anything else. Due to their smaller size, they are \",[\"$\",\"strong\",null,{\"children\":\"much cheaper to run\"}],\" than general-purpose LLMs,\\nand they can be deployed on more modest hardware.\"]}]\n16:[\"$\",\"p\",null,{\"children\":[\"More importantly, task-specific LLMs are \",[\"$\",\"strong\",null,{\"children\":\"trained on datasets that are relevant to the task at hand\"}],\". This means\\nthat they can leverage domain knowledge and perform much better than general-purpose LLMs on narrow tasks.\\nFor instance, a task-specific LLM for \",[\"$\",\"strong\",null,{\"children\":\"safety filtering\"}],\" would be trained on a dataset of safe and unsafe\\nmessages, as per the user's definition of safety, and would learn to identify patterns and features that are\\nindicative of unwanted content.\"]}]\n17:[\"$\",\"h2\",null,{\"children\":\"How Tanaos can help\"}]\n18:[\"$\",\"p\",null,{\"children\":[\"Our \",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Artifex Python library\"}],\" provides\\na simple and efficient way to create and use small-sized, task-specific LLMs for text classification tasks.\\nMost importantly, Artifex allows you to create task-specific LLMs \",[\"$\",\"strong\",null,{\"children\":\"without the need for any training data\"}],\".\\nYou simply describe how the model should behave, and it will be trained on synthetic data generated\\nfor that purpose.\"]}]\n19:[\"$\",\"p\",null,{\"children\":[\"The models created with Artifex are so lightweight that they can \",[\"$\",\"strong\",null,{\"children\":\"run locally or on small servers\\nwithout a GPU\"}],\", offloading simple tasks and reducing reliance on third-party LLM APIs.\"]}]\n1a:[\"$\",\"h2\",null,{\"children\":\"Give it a try\"}]\n1b:[\"$\",\"p\",null,{\"children\":"])</script><script>self.__next_f.push([1,"\"Artifex is free and open-source. Check it out on GitHub!\"}]\n1c:[\"$\",\"a\",null,{\"className\":\"btn btn-primary\",\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":[\"See Artifex on GitHub \",[\"$\",\"i\",null,{\"className\":\"bi bi-github ms-2\"}]]}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"b:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Working on text classification? Use a task-specific LLM. | Tanaos Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"keywords\",\"content\":\"task-specific LLM,AI,text classification,without training data,NLP,machine learning\"}],[\"$\",\"link\",\"2\",{\"rel\":\"canonical\",\"href\":\"undefined/blog/task-specific-llms/\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Working on text classification? Use a task-specific LLM.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"undefined/blog/task-specific-llms/\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"6\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:title\",\"content\":\"Working on text classification? Use a task-specific LLM.\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"10:\"$b:metadata\"\n"])</script></body></html>