1:"$Sreact.fragment"
2:I[58665,["87","static/chunks/0e762574-e97948326193efde.js","711","static/chunks/8e1d74a4-b11957bef1dd4fbf.js","754","static/chunks/754-c636fd1d5677d3bc.js","356","static/chunks/356-a1921fb11ab00663.js","177","static/chunks/app/layout-c77bb0cda7ec6360.js"],"Navbar"]
3:I[9766,[],""]
4:I[98924,[],""]
5:I[52619,["87","static/chunks/0e762574-e97948326193efde.js","754","static/chunks/754-c636fd1d5677d3bc.js","883","static/chunks/883-2b291bba2a595a03.js","356","static/chunks/356-a1921fb11ab00663.js","953","static/chunks/app/blog/%5Bslug%5D/page-e9f86361092ee138.js"],""]
b:I[57150,[],""]
:HL["/_next/static/media/0484562807a97172-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/8888a3826f4a3af4-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/eafabf029ad39a43-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/330df8a60483a06e.css","style"]
:HL["/_next/static/css/5388290d976a8ef2.css","style"]
:HL["/_next/static/css/c334275b0572c852.css","style"]
:HL["/_next/static/css/0fb36d5ac05ca721.css","style"]
:HL["/_next/static/css/5d643ef3b3193cbd.css","style"]
0:{"P":null,"b":"zpPn3Ib05nD7q0hoXIzkW","p":"","c":["","blog","cut-guardrail-costs",""],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","cut-guardrail-costs","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/330df8a60483a06e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/5388290d976a8ef2.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/c334275b0572c852.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__className_cc80f9","children":[["$","div",null,{"className":"app","children":[["$","$L2",null,{}],["$","main",null,{"className":"content","children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"Footer_footer__Ir1kR","children":["$","div",null,{"className":"Footer_footer-inner__mvqEE","children":[["$","div",null,{"className":"Footer_footer-top__kBSno","children":[["$","div",null,{"className":"Footer_footer-nav__NeXbd","children":[["$","$L5",null,{"href":"/","children":"Home"}],["$","$L5",null,{"href":"/blog/","children":"Blog"}],["$","a",null,{"href":"https://platform.tanaos.com","rel":"noreferrer","target":"_blank","children":"Platform"}],["$","a",null,{"href":"/models/ticket-classification/","children":"Ticket Classification"}],["$","a",null,{"href":"/models/contact-form-spam-filter/","children":"Contact Form Spam Filter"}],["$","a",null,{"href":"/models/email-intent-detection/","children":"Email Intent Detection"}],["$","a",null,{"href":"/models/chatbot-safety-moderation/","children":"Chatbot Safety & Moderation"}]]}],["$","div",null,{"className":"Footer_footer-social__kWIvl","children":[["$","a",null,{"href":"https://github.com/tanaos","rel":"noreferrer","target":"_blank","aria-label":"GitHub","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","children":["$undefined",[["$","path","0",{"d":"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8","children":[]}]]],"className":"$undefined","style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","a",null,{"href":"https://huggingface.co/tanaos","rel":"noreferrer","target":"_blank","aria-label":"Hugging Face","children":"ðŸ¤—"}]]}]]}],["$","div",null,{"className":"Footer_footer-divider__t_9Q_"}],["$","div",null,{"className":"Footer_footer-bottom__3DnO7","children":["$","span",null,{"children":["Â© ",2026," Tanaos. All rights reserved."]}]}]]}]}]]}],"$L6"]}]}]]}],{"children":["blog","$L7",{"children":[["slug","cut-guardrail-costs","d"],"$L8",{"children":["__PAGE__","$L9",{},null,false]},null,false]},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[41402,["87","static/chunks/0e762574-e97948326193efde.js","711","static/chunks/8e1d74a4-b11957bef1dd4fbf.js","754","static/chunks/754-c636fd1d5677d3bc.js","356","static/chunks/356-a1921fb11ab00663.js","177","static/chunks/app/layout-c77bb0cda7ec6360.js"],""]
e:I[24431,[],"OutletBoundary"]
10:I[15278,[],"AsyncMetadataOutlet"]
12:I[24431,[],"ViewportBoundary"]
14:I[24431,[],"MetadataBoundary"]
15:"$Sreact.suspense"
6:["$","$Lc",null,{"id":"ga-init","strategy":"afterInteractive","children":"\n                        window.dataLayer = window.dataLayer || [];\n                        function gtag(){dataLayer.push(arguments);}\n                        gtag('js', new Date());\n                        gtag('config', 'G-HRQ77GT2C8');\n                    "}]
7:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
8:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
9:["$","$1","c",{"children":["$Ld",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/0fb36d5ac05ca721.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/5d643ef3b3193cbd.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$Le",null,{"children":["$Lf",["$","$L10",null,{"promise":"$@11"}]]}]]}]
a:["$","$1","h",{"children":[null,[["$","$L12",null,{"children":"$L13"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L14",null,{"children":["$","div",null,{"hidden":true,"children":["$","$15",null,{"fallback":null,"children":"$L16"}]}]}]]}]
17:I[81356,["87","static/chunks/0e762574-e97948326193efde.js","754","static/chunks/754-c636fd1d5677d3bc.js","883","static/chunks/883-2b291bba2a595a03.js","356","static/chunks/356-a1921fb11ab00663.js","953","static/chunks/app/blog/%5Bslug%5D/page-e9f86361092ee138.js"],"Image"]
d:["$","article",null,{"className":"BlogPage_article__fv9gc","children":[["$","$L5",null,{"href":"/blog/","className":"BlogPage_back-link__G6xqk","children":[["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","children":["$undefined",[["$","path","0",{"fillRule":"evenodd","d":"M15 8a.5.5 0 0 0-.5-.5H2.707l3.147-3.146a.5.5 0 1 0-.708-.708l-4 4a.5.5 0 0 0 0 .708l4 4a.5.5 0 0 0 .708-.708L2.707 8.5H14.5A.5.5 0 0 0 15 8","children":[]}]]],"className":"$undefined","style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]," Back to blog"]}],["$","h1",null,{"className":"mt-4","children":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."}],["$","p",null,{"className":"BlogPage_subtitle__wqaGw","children":"Reduce your reliance on expensive API calls by offloading guardrail-specific queries to self-hosted, small guardrail models that don't need a GPU."}],["$","p",null,{"className":"BlogPage_date__VyN7U","children":"December 12, 2025"}],["$","$L17",null,{"className":"mt-5 mb-5 BlogPage_post-image__Du0zR","src":"/images/blog/cut-guardrail-costs.png","alt":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.","width":1200,"height":630,"unoptimized":true}],[["$","p",null,{"children":["It may or may not surprise you that, in most chatbots implemented through an LLM API, guardrail-related queries\naccount on average for ",["$","strong",null,{"children":"40% of total API costs"}],". And by the way, if you need a refresher on what\nguardrails are and why they are important, check out our previous\n",["$","$L5",null,{"href":"/blog/guardrail-models/","children":"post on guardrails"}],"."]}],"\n",["$","p",null,{"children":["Despite their high cost, guardrailing tasks are straightforward and easy to get right even for small models\nin the 0.1B parameters range (or even smaller). Making a call to an LLM API just to check whether a message is\nsafe is effectively ",["$","strong",null,{"children":"a waste of money"}],"."]}],"\n",["$","p",null,{"children":["The problem is all the more frustrating when you consider that any respectable chatbot should make ",["$","strong",null,{"children":"not one,\nbut two"}]," guardrail-related API calls for every user query: one to make sure the user query is safe, before\nit is fed to the chatbot, and the other to make sure that the chatbot output is safe, before showing it\nto the user."]}],"\n",["$","p",null,{"children":"Latency is another important factor to consider: every additional API call adds network latency by increasing\nthe number of round-trips to the API server."}],"\n",["$","h2",null,{"children":"So why don't people self-host their guardrail model?"}],"\n",["$","p",null,{"children":[["$","em",null,{"children":"\"If guardrailing tasks are so easy to get right even for small models...\""}],", you may ask, ",["$","em",null,{"children":"\"...then why\ndoesn't everybody train their own guardrail, self-host it on the same server as the chatbot backend\nand save themselves tons of money?\""}],"."]}],"\n",["$","p",null,{"children":["As is often the case in AI, ",["$","strong",null,{"children":"the problem is data"}],". While ",["$","em",null,{"children":"some"}]," of the content that guardrails\nshould guard against are common to most users (profanity, hate speech, violence or self-harm,\nadult content, prompt injection and jailbreak prevention...) a large portion of the guardrail's behavior\nreally ",["$","strong",null,{"children":"depends on each user's specific needs"}],": some users may need guardrails against legal or medical advice,\nothers may want to keep away from mentioning competitors, others may want to avoid political topics,\nand so on."]}],"\n",["$","p",null,{"children":["Guardrail models won't just magically know what they need to guard against: they need to be taught. However,\nteaching them requires ","$L18",", which is something that\nmost developers (or even companies) ","$L19","."]}],"\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e","\n","$L3f","\n","$L40","\n","$L41","\n","$L42","\n","$L43"]]}]
44:I[50270,["87","static/chunks/0e762574-e97948326193efde.js","754","static/chunks/754-c636fd1d5677d3bc.js","883","static/chunks/883-2b291bba2a595a03.js","356","static/chunks/356-a1921fb11ab00663.js","953","static/chunks/app/blog/%5Bslug%5D/page-e9f86361092ee138.js"],"CodeSnippet"]
18:["$","strong",null,{"children":"datasets with tens of thousands of labeled utterances"}]
19:["$","strong",null,{"children":"don't have"}]
1a:["$","h2",null,{"children":"Regular chatbot implementation"}]
1b:["$","p",null,{"children":"Let's suppose we have a little online store, and we want to develop a chatbot that acts as a\ncustomer assistant. The chatbot should help customers track their orders, answer inquiries on\navailable products and so on."}]
1c:["$","p",null,{"children":["Since most developers would implement a guardrailed chatbot entirely through an LLM API, that's the\nscenario we will be using as our starting point. We will employ ",["$","strong",null,{"children":"OpenAI's GPT-4.1 API"}],", but the\nsame principles apply to any other LLM API. The chatbot will be implemented as a ",["$","strong",null,{"children":"FastAPI web\nservice"}],", with a single ",["$","code",null,{"children":"/chat"}]," endpoint that receives user messages."]}]
1d:["$","p",null,{"children":"A bare-bone implementation may look like this:"}]
1e:["$","$L44",null,{"code":"from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR_API_KEY\")\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n  message: str\n\n@app.post(\"/chat\")\nasync def chat_endpoint(req: ChatRequest):\n\n  user_msg = req.message\n\n  system_prompt = \"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": user_msg}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  return {\"reply\": reply}\n"}]
1f:["$","h3",null,{"children":"Cost & latency assessment of guardrail-free chatbot"}]
20:["$","p",null,{"children":["As of the time of writing, the GPT-4.1 API costs $2 per 1M input tokens and $8 per 1M output tokens. Since the\nsystem prompt we provided is 30 tokens long, if we assume that the average user message is 20 tokens long and the\naverage chatbot response is 40 tokens long, the current chatbot implementation incurs a ",["$","strong",null,{"children":"cost of\n$0.42 per 1K user messages"}],", with an average measured ",["$","strong",null,{"children":"latency of 1.4 seconds"}],". Not too bad."]}]
21:["$","h3",null,{"children":"Adding guardrails"}]
22:["$","p",null,{"children":["But here's the catch: except for the high-level guardrails embedded into every OpenAI model, this simple\nimplementation ",["$","strong",null,{"children":"doesn't include any kind of guardrailing"}],". Our chatbot will do its best to answer user\nqueries, even when they are out-of-scope or potentially harmful."]}]
23:["$","p",null,{"children":["Say, for instance, that a user asks our chatbot to help him build a React component for his latest full-stack\nweb application, asks for medical advice, or tries to get the chatbot to reveal confidential information\nabout our online store. Such attempts are not only a potential ",["$","strong",null,{"children":"security threat and legal liability"}],", but\nwill ",["$","strong",null,{"children":"increase the overall cost"}]," of our chatbot, as the model will waste tokens trying to answer out-of-scope\nqueries."]}]
24:["$","p",null,{"children":["We could (and should), of course, refine the system prompt by instructing the chatbot to not answer\nany queries that are unrelated to the online store, but this ",["$","strong",null,{"children":"naive security mechanism would be insufficient"}],",\nsince system prompts can be bypassed with simple stratagems (prompt injection techniques, various versions of\n",["$","em",null,{"children":"\"forget all previous instructions and...\""}],"). In order to make it impossible the users to bypass the\nsecurity mechanism, any effective guardrail must be implemented as an ",["$","strong",null,{"children":"external component"}],"."]}]
25:["$","p",null,{"children":["In fact, to be safe we should implement ",["$","strong",null,{"children":"not one, but two guardrails"}],": one for the user query, before it is\npassed to the chatbot (to ensure the user does not input any harmful or out-of-scope content), and one for\nthe chatbot output, before it is displayed to the user (to ensure the chatbot doesn't output any harmful or\nout-of-scope content)."]}]
26:["$","p",null,{"children":"Let's add the two-levels guardrailing system to the our chatbot:"}]
45:T9d7,from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")

app = FastAPI()

class ChatRequest(BaseModel):
  message: str
  
def input_guardrail(message: str):

  safe_topics = ["customer service", "product information", "order status", "returns", "shipping"]
  prompt = f"You are a guardrail model that ensures the user's message is appropriate for a customer service chatbot. Only allow messages related to the following topics: " + ", ".join(safe_topics) + ". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'."

  response = client.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": prompt},
          {"role": "user", "content": message}
      ]
  )

  reply = response.choices[0].message.content
  return reply

def output_guardrail(message: str):

  unsafe_content = ["hate speech", "personal attacks", "sensitive information", "inappropriate language"]
  prompt = f"You are a guardrail model that ensures the chatbot's response is appropriate for a customer service context. Messages containing the following types of content are considered inappropriate: " + ", ".join(unsafe_content) + ". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'."

  response = client.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": prompt},
          {"role": "user", "content": message}
      ]
  )

  reply = response.choices[0].message.content
  return reply

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
  
  user_msg = req.message
  
  # Input Guardrail Check
  input_check = input_guardrail(user_msg)
  if input_check == "unsafe":
      return {"reply": "I'm sorry, but I cannot help you with that."}
  
  # Generate Chatbot Response
  system_prompt = "You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services."

  response = client.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": user_msg}
      ]
  )

  reply = response.choices[0].message.content
  
  # Output Guardrail Check
  output_check = output_guardrail(reply)
  if output_check == "unsafe":
      return {"reply": "I'm sorry, but I cannot help you with that."}
  
  return {"reply": reply}
27:["$","$L44",null,{"code":"$45"}]
28:["$","h3",null,{"children":"Cost & latency assessment of guardrailed chatbot"}]
29:["$","p",null,{"children":["Boom, our chatbot is now much safer, but its ",["$","strong",null,{"children":"cost has increased dramatically"}],". For every user message, we\nare now making not one, but three separate requests to OpenAI's API: one for the input guardrail, one for chat\ncompletion, one for the output guardrail."]}]
2a:["$","p",null,{"children":["The price increase is not even the only issue here: ",["$","strong",null,{"children":"latency has gone up significantly"}]," too. Except for cases\nin which the conversation stops at the input guardrail (if the user query is unsafe, that particular\nconversation stops there and never makes it to the chat completion or output guardrail), all user messages now go\nthrough three separate round-trips to the OpenAI servers instead of just one."]}]
2b:["$","p",null,{"children":["To put things into perspective, this new implementation consumes, on average, an additional 130 input tokens\nand 2 output tokens per user message. Assuming the same average number of tokens per user message as before, this\ntranslates into an overall cost of $0.7 per 1K user messages. Since the cost of the guardrail-free chatbot\nwas $0.42 per 1K user messages, the guardrailed chatbot is around ",["$","strong",null,{"children":"67% more expensive"}],". Things don't\nlook better latency-wise: the new average measured latency is 3.4 seconds, which means our\nguardrailed chatbot is ",["$","strong",null,{"children":"2.5x slower"}]," than the guardrai-free version."]}]
2c:["$","$L17",null,{"src":"/images/blog/guardrail-cost-chart.png","alt":"Guardrail cost chart","width":0,"height":0,"style":{"width":"100%","height":"auto"},"unoptimized":true,"className":"mb-5"}]
2d:["$","h2",null,{"children":"Offloading guardrail tasks to a self-hosted model with Artifex"}]
2e:["$","p",null,{"children":["We now come to the main point of this tutorial. If only we had a way to offload guardrail-related\nqueries to a small guardrail model that runs on the same server as the chatbot backend, instead of\nsending them to OpenAI's API, we could reduce the number of API calls by two-thirds,\nthereby ",["$","strong",null,{"children":"cutting costs and latency"}]," by a significant amount."]}]
2f:["$","p",null,{"children":["Such a solution is, in fact, possible with ",["$","a",null,{"href":"https://github.com/tanaos/artifex","target":"_blank","rel":"noreferrer","children":"Artifex"}],".\nArtifex is an open-source Python library for using and fine-tuning small, task-specific LLMs without the need for labeled\ndata or GPUs."]}]
30:["$","p",null,{"children":"We will again create two separate guardrails, one for the input message and one for the output message, then\nreplace the API-based guardrails with the ones running locally."}]
31:["$","p",null,{"children":"First of all, install the Artifex library with"}]
32:["$","$L44",null,{"code":"pip install artifex"}]
33:["$","p",null,{"children":"Creating a guardrail model with Artifex is as simple as the following:"}]
34:["$","$L44",null,{"code":"from artifex import Artifex\n\ngr = Artifex().guardrail\n\nmodel_output_path = \"./input_guardrail/\"\n\ngr.train(\n  instructions=[\n      \"Queries related to customer service, product information, order status, returns and shipping are allowed.\",\n      \"Everything else is not allowed.\"\n  ],\n  output_path=model_output_path\n)\n"}]
35:["$","p",null,{"children":["There. All we have done is passing to the ",["$","code",null,{"children":"instructions"}]," argument of ",["$","code",null,{"children":"Artifex().guardrail.train()"}]," the list of\nallowed and not allowed queries. Read the two strings inside the list passed as the ",["$","code",null,{"children":"instructions"}],"\nargument: they describe the same allowed and unallowed content that we passed to the\nAPI-based guardrail's system prompt earlier."]}]
36:["$","p",null,{"children":["The training process will take a few minutes. Once it's done, the model will\nbe saved to the ",["$","code",null,{"children":"./input_guardrail/"}]," folder, together with the synthetic dataset that was generated on-the-fly\nduring training, based on the instructions we provided."]}]
37:["$","p",null,{"children":"Once the input guardrail model is ready, we can create the output guardrail model in a similar way:"}]
38:["$","$L44",null,{"code":"from artifex import Artifex\n\ngr = Artifex().guardrail\n\nmodel_output_path = \"./output_guardrail/\"\n\ngr.train(\n  instructions=[\n      \"Responses that contain hate speech, personal attacks, sensitive information or inappropriate language are not allowed.\",\n      \"Everything else is allowed.\"\n  ],\n  output_path=model_output_path\n)\n"}]
39:["$","p",null,{"children":"Now that both guardrail models are ready, we can modify our FastAPI chatbot implementation to use\nthe guardrail models we just created instead of the API-based ones:"}]
46:T51b,from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI
from artifex import Artifex

client = OpenAI(api_key="YOUR_API_KEY")

app = FastAPI()

class ChatRequest(BaseModel):
  message: str

input_guardrail = Artifex().guardrail
output_guardrail = Artifex().guardrail

input_guardrail.load("./input_guardrail")
output_guardrail.load("./output_guardrail")

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):

  user_msg = req.message

  # Input Guardrail Check
  input_check = input_guardrail(user_msg)[0].label
  if input_check == "unsafe":
      return {"reply": "I'm sorry, but I cannot help you with that."}

  # Generate Chatbot Response
  system_prompt = "You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services."

  response = client.chat.completions.create(
      model="gpt-4.1",
      messages=[
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": user_msg}
      ]
  )

  reply = response.choices[0].message.content

  # Output Guardrail Check
  output_check = output_guardrail(reply)[0].label
  if output_check == "unsafe":
      return {"reply": "I'm sorry, but I cannot help you with that."}

  return {"reply": reply}
3a:["$","$L44",null,{"code":"$46"}]
3b:["$","p",null,{"children":["Our code is even simpler than before. We just instantiate two guardrail models with\n",["$","code",null,{"children":"Artifex().guardrail"}],", then load the two fine-tuned guardrails we created earlier with the ",["$","code",null,{"children":"load()"}],"\nmethod. We can then remove the two ",["$","code",null,{"children":"input_guardrail()"}]," and ",["$","code",null,{"children":"output_guardrail()"}]," functions entirely,\nreplacing them with calls to our newly loaded input and output guardrails. The rest is exactly\nthe same as before."]}]
3c:["$","p",null,{"children":["For more information on how to train, load and perform inference with guardrail models with Artifex, check out\nthe ",["$","a",null,{"href":"https://docs.tanaos.com/artifex/guardrail/train/","target":"_blank","rel":"noreferrer","children":"\nArtifex documentation"}],"."]}]
3d:["$","h3",null,{"children":"Cost & latency assessment of chatbot that uses self-hosted guardrails"}]
3e:["$","p",null,{"children":["Since all guardrail-related inference is now happening entirely on our CPU, OpenAI will not bill\nus for it. Our chatbot cost is therefore back to its original level (prior to adding the API-based\nguardrails), which is $0.42 per 1K user messages, while its average measured latency is 1.6 seconds.\nThis translates to a ",["$","strong",null,{"children":"43% reduction in costs and a 53% reduction in latency"}]," compared to the\nchatbot that uses API-based guardrails."]}]
3f:["$","$L17",null,{"src":"/images/blog/local-guardrail-cost-chart.png","alt":"Guardrail cost chart","width":0,"height":0,"style":{"width":"100%","height":"auto"},"unoptimized":true,"className":"mb-5"}]
40:["$","h2",null,{"children":"Wrapping up"}]
41:["$","p",null,{"children":["In this tutorial, we have seen how to use Artifex to create small, task-specific guardrail\nmodels that run on the same server as the chatbot backend, thereby ",["$","strong",null,{"children":"cutting chatbot costs by\n43% and latency by 53%"}],", by offloading guardrail-related queries to the self-hosted models."]}]
42:["$","p",null,{"children":"This approach is not limited to guardrail tasks only: any kind of specialized text\nclassification task that would otherwise require expensive API calls can be offloaded to a small,\nlocally running model created with Artifex, thereby reducing costs and latency across the board."}]
43:["$","p",null,{"children":["If you want to learn more about Artifex and how to use it, check out its\n",["$","a",null,{"href":"https://github.com/tanaos/artifex","target":"_blank","rel":"noreferrer","children":"GitHub repository"}]," and\n",["$","a",null,{"href":"$undefined","target":"_blank","rel":"noreferrer","children":"documentation"}],"."]}]
13:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
f:null
11:{"metadata":[["$","title","0",{"children":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail. | Tanaos Blog"}],["$","meta","1",{"name":"keywords","content":"task-specific LLM,offline NLP,text classification,without training data,NLP,Artifex,0.4.0"}],["$","link","2",{"rel":"canonical","href":"undefined/blog/cut-guardrail-costs/"}],["$","meta","3",{"property":"og:title","content":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."}],["$","meta","4",{"property":"og:url","content":"undefined/blog/cut-guardrail-costs/"}],["$","meta","5",{"property":"og:image","content":"https://tanaos.com/images/blog/cut-guardrail-costs.png"}],["$","meta","6",{"property":"og:image:width","content":"1200"}],["$","meta","7",{"property":"og:image:height","content":"630"}],["$","meta","8",{"property":"og:image:alt","content":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."}],["$","meta","9",{"property":"og:type","content":"article"}],["$","meta","10",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","11",{"name":"twitter:title","content":"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."}],["$","meta","12",{"name":"twitter:image","content":"https://tanaos.com/images/blog/cut-guardrail-costs.png"}]],"error":null,"digest":"$undefined"}
16:"$11:metadata"
