<!DOCTYPE html><!--wjy0ipGguzpu_ljKqTiES--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/0484562807a97172-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/8888a3826f4a3af4-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/eafabf029ad39a43-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/a60dd70027cfe1b8.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/9738dfd5a8ab60ee.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c334275b0572c852.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e6579c774bfbe783.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/6e18e0dd86c4d89f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-946677802ac40577.js"/><script src="/_next/static/chunks/4bd1b696-409494caf8c83275.js" async=""></script><script src="/_next/static/chunks/255-8db1c35057a14be6.js" async=""></script><script src="/_next/static/chunks/main-app-4ffd0bfd209b03a4.js" async=""></script><script src="/_next/static/chunks/553-2a38a743b7160be3.js" async=""></script><script src="/_next/static/chunks/app/layout-9e08d8eb832d22a9.js" async=""></script><script src="/_next/static/chunks/159-6a7d7131ac72839c.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-58e2685f9ffee653.js" async=""></script><meta name="next-size-adjust" content=""/><title>Cut your chatbot costs and latency by 40% using local Guardrail models. | Tanaos Blog</title><meta name="keywords" content="task-specific LLM,offline NLP,text classification,without training data,NLP,Artifex,0.4.0"/><link rel="canonical" href="undefined/blog/cut-guardrail-costs/"/><meta property="og:title" content="Cut your chatbot costs and latency by 40% using local Guardrail models."/><meta property="og:url" content="undefined/blog/cut-guardrail-costs/"/><meta property="og:image" content="https://tanaos.com/images/blog/cut-guardrail-costs.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Cut your chatbot costs and latency by 40% using local Guardrail models."/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Cut your chatbot costs and latency by 40% using local Guardrail models."/><meta name="twitter:image" content="https://tanaos.com/images/blog/cut-guardrail-costs.png"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_cc80f9"><div hidden=""><!--$--><!--/$--></div><div class="app"><div class="row Navbar_navbar__3CvTR m-0 false"><div class="m-0 p-0 Navbar_navbar-large-devices__rsA5K"><div class="col m-0 p-0 text-start"><img alt="Create task-specific LLMs for NLP and Text Classification | Tanaos" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="Navbar_logo__RSROF" style="color:transparent" src="/images/logo.png"/></div><div class="col m-0 p-0 text-end"><div class="Navigation_navigation__Eln2g"><a class="mt-4 mt-md-0 ms-md-4 false" href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Platform</a><a class="btn btn-white mt-4 mt-md-0 ms-md-4" href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">See on GitHub <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-github ms-1"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"></path></svg></a></div></div></div><div class="Navbar_navbar-small-devices__w8Bl9 d-md-none m-0 p-0 d-flex align-items-center"><div class="col-10 m-0 p-0 text-start"><img alt="Create task-specific LLMs for NLP and Text Classification | Tanaos" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="Navbar_logo__RSROF" style="color:transparent" src="/images/logo.png"/></div><div class="col-2 m-0 p-0 text-end"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-list Navbar_navbar-toggle-icon__TXjJe"><path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"></path></svg></div></div><div class="d-md-none Navbar_navbar-collapse__09PzW text-start false"><div class="mt-4"><div class="Navigation_navigation__Eln2g"><a class="mt-4 mt-md-0 ms-md-4 false" href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank" class="mt-4 mt-md-0 ms-md-4">Platform</a><a class="btn btn-white mt-4 mt-md-0 ms-md-4" href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">See on GitHub <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-github ms-1"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"></path></svg></a></div></div></div></div><main class="content"><article class="BlogPage_article__fv9gc"><a class="BlogPage_back-link__G6xqk" href="/blog/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-arrow-left"><path fill-rule="evenodd" d="M15 8a.5.5 0 0 0-.5-.5H2.707l3.147-3.146a.5.5 0 1 0-.708-.708l-4 4a.5.5 0 0 0 0 .708l4 4a.5.5 0 0 0 .708-.708L2.707 8.5H14.5A.5.5 0 0 0 15 8"></path></svg> Back to blog</a><h1 class="mt-4">Cut your chatbot costs and latency by 40% using local Guardrail models.</h1><p class="BlogPage_subtitle__wqaGw">Use our Artifex library to reduce your reliance on expensive API calls by offloading guardrail-specific queries to a small model that runs locally.</p><p class="BlogPage_date__VyN7U">December 12, 2025</p><img alt="Cut your chatbot costs and latency by 40% using local Guardrail models." loading="lazy" width="1200" height="630" decoding="async" data-nimg="1" class="mt-5 mb-5 BlogPage_post-image__Du0zR" style="color:transparent" src="/images/blog/cut-guardrail-costs.png"/><p><a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">Artifex</a> is our Python library that
allows you to use and fine-tune small, task-specific LLMs without the need for training data, all on a
CPU.</p>
<p>In this tutorial, we will see how to use Artifex to <strong>drastically reduce chatbot costs</strong>, by offloading
guardrail-specific queries to a local guardrail model created with Artifex.</p>
<h2>Chatbot costs</h2>
<p>It may or may not surprise you that, in most chatbots implemented through an LLM API, guardrail-related queries
often account on averafe for <strong>40% of total API costs</strong>. And by the way, if you need a refresher on what
guardrails are and why they are important, check out our previous
<a href="/blog/guardrail-models/">post on guardrails</a>.</p>
<p>Despite their high cost, guardrailing tasks are straightforward and easy to get right even for small models
in the 0.1B parameters range (or even smaller). Making a call to an LLM API just to check whether a message is
safe is effectively <strong>a waste of money</strong>.</p>
<p>The problem is all the more frustrating when you consider that any respectable chatbot should make <strong>not one,
but two</strong> guardrail-related API calls for every user query: one to make sure the user query is safe, before
it is fed to the chatbot, and the other to make sure that the chatbot output is safe, before showing it
to the user.</p>
<p>Latency is another important factor to consider: every additional API call adds network latency by increasing
the number of round-trips to the API server.</p>
<h2>So why don&#x27;t people use local chatbots?</h2>
<p><em>&quot;If guardrailing tasks are so easy to get right even for small models...&quot;</em>, you may ask, <em>&quot;...then why
doesn&#x27;t everybody train their own little guardrail and save themselves tons of money?&quot;</em>.</p>
<p>As is often the case in AI, <strong>the problem is data</strong>. While <em>some</em> of the content that guardrails
should guard against are common to most users — profanity, hate speech, violence or self-harm,
adult content, prompt injection and jailbreak prevention... — a large portion of the guardrail&#x27;s behavior
really <strong>depends on each user&#x27;s specific needs</strong>: some users may need guardrails against legal or medical advice,
others may want to keep away from mentioning competitors, others may want to avoid political topics,
and so on.</p>
<p>Guardrail models won&#x27;t just magically know what they need to guard against: they need to be taught. However,
teaching them requires <strong>datasets with tens of thousands of labeled utterances</strong>, which is something that
most developers (or even companies) <strong>don&#x27;t have</strong>.</p>
<h2>The solution: Artifex</h2>
<p>Our <a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">Artifex library</a>
overcomes this problem by allowing you to fine-tune a small, general-purpose guardrail model — one
with a few, very general instructions already in place (such as profanity and hate speech detection)
— <strong>without the need to provide any training dataset</strong>. You simply describe what the guardrail
model should guard against, and Artifex will fine-tune the general-purpose model on <strong>synthetic
data generated on-the-fly</strong> based on your requirements.</p>
<p>Let&#x27;s see how to do that.</p>
<h2>Regular chatbot implementation</h2>
<p>Let&#x27;s suppose we have a little online store, and we want to develop a chatbot that acts as a
customer assistant. The chatbot should help customers track their orders, answer inquiries on
available products and so on.</p>
<p>We will be using OpenAI&#x27;s GPT-4.1 API, but the same principles apply to any other LLM API. The chatbot will
be implemented as a FastAPI web service, with a single <code>/chat</code> endpoint that receives user messages.</p>
<p>A bare-bone implementation may look like this:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>
<span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<h3>Cost &amp; latency assessment of guardrail-free model</h3>
<p>As of the time of writing, the GPT-4.1 API costs $2 per 1M input tokens and $8 per 1M output tokens. Since the
system prompt we provided is 30 tokens long, if we assume that the average user message is 20 tokens long and the
average chatbot response is 40 tokens long, the current chatbot implementation incurs a <strong>cost of
$0.42 per 1K user messages</strong>, with an average measured <strong>latency of 1.4 seconds</strong>. Not too bad.</p>
<h3>Adding guardrails</h3>
<p>But here&#x27;s the catch: except for the high-level guardrails embedded into every OpenAI model, this simple
implementation <strong>doesn&#x27;t include any kind of guardrailing</strong>. Our chatbot will do its best to answer user
queries, even when they are out-of-scope or potentially harmful.</p>
<p>Say, for instance, that a user asks our chatbot to help him build a React component for his latest full-stack
web application, asks for medical advice, or tries to get the chatbot to reveal confidential information
about our online store. Such attempts are not only a potential <strong>security threat and legal liability</strong>, but
will <strong>increase the overall cost</strong> of our chatbot, as the model will waste tokens trying to answer out-of-scope
queries.</p>
<p>We could (and should), of course, refine the system prompt by instructing the chatbot to not answer
any queries that are unrelated to the online store, but this <strong>simple security mechanism would be insufficient</strong>,
since system prompts can be bypassed with simple stratagems (prompt injection techniques, various versions of
<em>&quot;forget all previous instructions and...&quot;</em>). In order to make it impossible for the user to bypass the
security mechanism, any effective guardrail must be implemented as an <strong>external component</strong>.</p>
<p>In fact, to be safe we should implement <strong>not one, but two guardrails</strong>: one to the user query, before it is
passed to the chatbot (to ensure the user does not input any harmful or out-of-scope content), and one to
the chatbot output, before it is displayed to the user (to ensure the chatbot doesn&#x27;t output any harmful or
out-of-scope content).</p>
<p>Let&#x27;s add the two-levels guardrailing system to the our chatbot:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>  
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  safe_topics </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;customer service&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;product information&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;order status&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;returns&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;shipping&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token string-interpolation" style="color:hsl(95, 38%, 62%)">f&quot;You are a guardrail model that ensures the user&#x27;s message is appropriate for a customer service chatbot. Only allow messages related to the following topics: &quot;</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;, &quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>join</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>safe_topics</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;. If the message is inappropriate, respond with &#x27;unsafe&#x27;. Otherwise, respond with &#x27;safe&#x27;.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> message</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> reply
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  unsafe_content </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;hate speech&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;personal attacks&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;sensitive information&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;inappropriate language&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token string-interpolation" style="color:hsl(95, 38%, 62%)">f&quot;You are a guardrail model that ensures the chatbot&#x27;s response is appropriate for a customer service context. Messages containing the following types of content are considered inappropriate: &quot;</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;, &quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>join</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>unsafe_content</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;. If the message is inappropriate, respond with &#x27;unsafe&#x27;. Otherwise, respond with &#x27;safe&#x27;.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> message</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> reply
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>  
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>  
<span>  </span><span class="token" style="color:#aaa"># Input Guardrail Check</span><span>
</span><span>  input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>  
<span>  </span><span class="token" style="color:#aaa"># Generate Chatbot Response</span><span>
</span><span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span>  
<span>  </span><span class="token" style="color:#aaa"># Output Guardrail Check</span><span>
</span><span>  output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>reply</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>  
<span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<h3>Cost &amp; latency assessment of guardrailed model</h3>
<p>Boom, our chatbot is now much safer, but its <strong>cost has increased dramatically</strong>. For every user message, we
are making not one, but three separate requests to OpenAI&#x27;s API: one for the input guardrail, one for chat
completion, one for the output guardrail.</p>
<p>The price increase is not even the only issue here: <strong>latency has gone up significantly</strong> too. Except for cases
in which the conversation stops at the input guardrail (if the user query is unsafe, that particular
conversation stops there and never makes it to the chat completion or output guardrail), all user messages now go
through three separate round-trips to the OpenAI servers instead of just one.</p>
<p>To put things into perspective, this new implementation consumes, on average, an additional 130 input tokens
and 2 output tokens per user message. Assuming the same average number of tokens per user message as before, this
translates into an overall cost of $0.7 per 1K user messages. Since the cost of the guardrail-free chatbot
was $0.42 per 1K user messages, the guardrailed chatbot is around <strong>67% more expensive</strong>. Things don&#x27;t
look better latency-wise: the new average measured latency is 3.4 seconds, which means our
guardrailed chatbot is <strong>2.5x slower</strong> than the guardrai-free version.</p>
<img alt="Guardrail cost chart" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/images/blog/guardrail-cost-chart.png"/>
<p>To be fair, since our guardrail&#x27;s system prompt is very short (around 30 tokens, while real-world system prompts
are usually many times longer), our estimate on cost and latency increase is a bit of a <strong>worst-case scenario</strong>.
As the number of tokens in the system prompt rises relative to the number of tokens in the guardrail prompt,
the cost and latency increase will be less pronounced. Based on our experiments with various system and guardrail
prompts, we found that adding guardrails increments cost and latency by <strong>anywhere between 35% and 70%</strong> depending
on the number of tokens used.</p>
<h2>Offloading guardrail tasks to a local model with Artifex</h2>
<p>We now come to the main point of this tutorial. If only we had a way to offload guardrail-related queries to a
small guardrail model that runs locally, instead of sending them to OpenAI&#x27;s API, we could reduce the number of
API calls by two-thirds, thereby <strong>cutting costs and latency</strong> by a significant amount.</p>
<p>Such a solution is, in fact, possible with <a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">Artifex</a>.
Artifex allows us to create a small guardrail model based on our requirements in under five minutes and without
any training dataset.</p>
<p>We will again create two separate guardrails, one for the input message and one for the output message, then
replace the API-based guardrails with the ones running locally.</p>
<p>First of all, install the Artifex library with</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>pip install artifex</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Creating a guardrail model with Artifex is as simple as the following:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>gr </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>model_output_path </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./input_guardrail/&quot;</span><span>
</span>
<span>gr</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>train</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>  instructions</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Queries related to customer service, product information, order status, returns and shipping are allowed.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Everything else is not allowed.&quot;</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>  output_path</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span>model_output_path
</span><span></span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>There. All we have done is passing to the <code>instructions</code> argument of <code>Artifex().guardrail.train()</code> the list of
allowed and not allowed queries. The training process will take a few minutes. Once it&#x27;s done, the model will
be saved to the <code>./input_guardrail/</code> folder, together with the synthetic dataset that was generated on-the-fly
during training, based on the instructions we provided.</p>
<p>Once the input guardrail model is ready, we can create the output guardrail model in a similar way:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>gr </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>model_output_path </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./output_guardrail/&quot;</span><span>
</span>
<span>gr</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>train</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>  instructions</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Responses that contain hate speech, personal attacks, sensitive information or inappropriate language are not allowed.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Everything else is allowed.&quot;</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>  output_path</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span>model_output_path
</span><span></span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Now that we have both guardrail models ready, we can modify our FastAPI chatbot implementation to use the local
models instead of the API-based ones:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>
<span>input_guardrail </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span><span>output_guardrail </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>load</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./input_guardrail&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>load</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./output_guardrail&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>
<span>  </span><span class="token" style="color:#aaa"># Input Guardrail Check</span><span>
</span><span>  input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>label
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>
<span>  </span><span class="token" style="color:#aaa"># Generate Chatbot Response</span><span>
</span><span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span>
<span>  </span><span class="token" style="color:#aaa"># Output Guardrail Check</span><span>
</span><span>  output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>reply</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>label
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>
<span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="1em" height="1em" fill="currentColor" class="bi bi-clipboard"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Our code is even simpler than before. We just instantiate two guardrail models with
<code>Artifex().guardrail</code>, then load the two fine-tuned guardrails we created earlier with the <code>load()</code>
method. We can then remove the two <code>input_guardrail()</code> and <code>output_guardrail()</code> functions entirely,
replacing them with calls to our newly loaded input and output guardrails. For more information on
how to train, load and perform inference with guardrail models with Artifex, check out
the <a href="https://docs.tanaos.com/artifex/guardrail/train/" target="_blank" rel="noreferrer">
Artifex documentation</a>.</p>
<h3>Cost &amp; latency assessment of model that uses local guardrails</h3>
<p>Since all guardrail-related inference is now happening entirely on our CPU, OpenAI will not bill
us for it. Our chatbot cost is therefore back to its original level (prior to adding the API-based
guardrails), which is $0.42 per 1K user messages, while its average measured latency is 1.6 seconds.
This translates to a <strong>43% reduction in costs and a 53% reduction in latency</strong> compared to the
chatbot that uses API-based guardrails.</p>
<img alt="Guardrail cost chart" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="mb-5" style="color:transparent;width:100%;height:auto" src="/images/blog/local-guardrail-cost-chart.png"/>
<h2>Wrapping up</h2>
<p>In this tutorial, we have seen how to use Artifex to create small, task-specific guardrail
models that run locally on our CPU, thereby <strong>drastically cutting chatbot costs and latency</strong> by
offloading guardrail-related queries to the local models.</p>
<p>This approach is not limited to guardrail tasks only: any kind of specialized text
classification task that would otherwise require expensive API calls can be offloaded to a small,
locally running model created with Artifex, thereby reducing costs and latency across the board.</p>
<p>If you want to learn more about Artifex and how to use it, check out our
<a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">GitHub repository</a> and
<a target="_blank" rel="noreferrer">documentation</a>.</p></article><!--$--><!--/$--></main><div class="row Footer_footer__Ir1kR m-0 align-items-center"><div class="col-12 col-md-4 m-0 p-0 text-start">Copyright © <!-- -->2025<!-- --> Tanaos</div><div class="col-md-4 text-md-center d-none d-md-block">Tanaos</div><div class="col-12 col-md-4 text-md-end mt-4 mt-md-0 Footer_footer-links__9Skqm m-0 p-0"><a href="/">Home</a><a href="/blog/">Blog</a><a href="https://docs.tanaos.com/artifex/" rel="noreferrer" target="_blank">Docs</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank">Platform</a></div></div></div><script src="/_next/static/chunks/webpack-946677802ac40577.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[47105,[\"553\",\"static/chunks/553-2a38a743b7160be3.js\",\"177\",\"static/chunks/app/layout-9e08d8eb832d22a9.js\"],\"Navbar\"]\n3:I[9766,[],\"\"]\n4:I[98924,[],\"\"]\n5:I[52619,[\"553\",\"static/chunks/553-2a38a743b7160be3.js\",\"159\",\"static/chunks/159-6a7d7131ac72839c.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2685f9ffee653.js\"],\"\"]\n6:I[41402,[\"553\",\"static/chunks/553-2a38a743b7160be3.js\",\"177\",\"static/chunks/app/layout-9e08d8eb832d22a9.js\"],\"\"]\n8:I[24431,[],\"OutletBoundary\"]\na:I[15278,[],\"AsyncMetadataOutlet\"]\nc:I[24431,[],\"ViewportBoundary\"]\ne:I[24431,[],\"MetadataBoundary\"]\nf:\"$Sreact.suspense\"\n11:I[57150,[],\"\"]\n:HL[\"/_next/static/media/0484562807a97172-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/8888a3826f4a3af4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/eafabf029ad39a43-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/a60dd70027cfe1b8.css\",\"style\"]\n:HL[\"/_next/static/css/9738dfd5a8ab60ee.css\",\"style\"]\n:HL[\"/_next/static/css/c334275b0572c852.css\",\"style\"]\n:HL[\"/_next/static/css/e6579c774bfbe783.css\",\"style\"]\n:HL[\"/_next/static/css/6e18e0dd86c4d89f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"wjy0ipGguzpu-ljKqTiES\",\"p\":\"\",\"c\":[\"\",\"blog\",\"cut-guardrail-costs\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"cut-guardrail-costs\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a60dd70027cfe1b8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/9738dfd5a8ab60ee.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c334275b0572c852.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_cc80f9\",\"children\":[[\"$\",\"div\",null,{\"className\":\"app\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"content\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"div\",null,{\"className\":\"row Footer_footer__Ir1kR m-0 align-items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-12 col-md-4 m-0 p-0 text-start\",\"children\":[\"Copyright © \",2025,\" Tanaos\"]}],[\"$\",\"div\",null,{\"className\":\"col-md-4 text-md-center d-none d-md-block\",\"children\":\"Tanaos\"}],[\"$\",\"div\",null,{\"className\":\"col-12 col-md-4 text-md-end mt-4 mt-md-0 Footer_footer-links__9Skqm m-0 p-0\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"children\":\"Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.tanaos.com/artifex/\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Docs\"}],[\"$\",\"a\",null,{\"href\":\"https://platform.tanaos.com\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Platform\"}]]}]]}]]}],[\"$\",\"$L6\",null,{\"id\":\"ga-init\",\"strategy\":\"afterInteractive\",\"children\":\"\\n                        window.dataLayer = window.dataLayer || [];\\n                        function gtag(){dataLayer.push(arguments);}\\n                        gtag('js', new Date());\\n                        gtag('config', 'G-HRQ77GT2C8');\\n                    \"}]]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"cut-guardrail-costs\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e6579c774bfbe783.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6e18e0dd86c4d89f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":[\"$L9\",[\"$\",\"$La\",null,{\"promise\":\"$@b\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Le\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$f\",null,{\"fallback\":null,\"children\":\"$L10\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:I[81356,[\"553\",\"static/chunks/553-2a38a743b7160be3.js\",\"159\",\"static/chunks/159-6a7d7131ac72839c.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2685f9ffee653.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"article\",null,{\"className\":\"BlogPage_article__fv9gc\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"className\":\"BlogPage_back-link__G6xqk\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 16 16\",\"width\":\"1em\",\"height\":\"1em\",\"fill\":\"currentColor\",\"className\":\"bi bi-arrow-left\",\"children\":[null,[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"d\":\"M15 8a.5.5 0 0 0-.5-.5H2.707l3.147-3.146a.5.5 0 1 0-.708-.708l-4 4a.5.5 0 0 0 0 .708l4 4a.5.5 0 0 0 .708-.708L2.707 8.5H14.5A.5.5 0 0 0 15 8\"}]]}],\" Back to blog\"]}],[\"$\",\"h1\",null,{\"className\":\"mt-4\",\"children\":\"Cut your chatbot costs and latency by 40% using local Guardrail models.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_subtitle__wqaGw\",\"children\":\"Use our Artifex library to reduce your reliance on expensive API calls by offloading guardrail-specific queries to a small model that runs locally.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_date__VyN7U\",\"children\":\"December 12, 2025\"}],[\"$\",\"$L12\",null,{\"className\":\"mt-5 mb-5 BlogPage_post-image__Du0zR\",\"src\":\"/images/blog/cut-guardrail-costs.png\",\"alt\":\"Cut your chatbot costs and latency by 40% using local Guardrail models.\",\"width\":1200,\"height\":630,\"unoptimized\":true}],[[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"Artifex\"}],\" is our Python library that\\nallows you to use and fine-tune small, task-specific LLMs without the need for training data, all on a\\nCPU.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In this tutorial, we will see how to use Artifex to \",[\"$\",\"strong\",null,{\"children\":\"drastically reduce chatbot costs\"}],\", by offloading\\nguardrail-specific queries to a local guardrail model created with Artifex.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Chatbot costs\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"It may or may not surprise you that, in most chatbots implemented through an LLM API, guardrail-related queries\\noften account on averafe for \",[\"$\",\"strong\",null,{\"children\":\"40% of total API costs\"}],\". And by the way, if you need a refresher on what\\nguardrails are and why they are important, check out our previous\\n\",[\"$\",\"$L5\",null,{\"href\":\"/blog/guardrail-models/\",\"children\":\"post on guardrails\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Despite their high cost, guardrailing tasks are straightforward and easy to get right even for small models\\nin the 0.1B parameters range (or even smaller). Making a call to an LLM API just to check whether a message is\\nsafe is effectively \",[\"$\",\"strong\",null,{\"children\":\"a waste of money\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The problem is all the more frustrating when you consider that any respectable chatbot should make \",[\"$\",\"strong\",null,{\"children\":\"not one,\\nbut two\"}],\" guardrail-related API calls for every user query: one to make sure the user query is safe, before\\nit is fed to the chatbot, and the other to make sure that the chatbot output is safe, before showing it\\nto the user.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Latency is another important factor to consider: every additional API call adds network latency by increasing\\nthe number of round-trips to the API server.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"So why don't people use local chatbots?\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"\\\"If guardrailing tasks are so easy to get right even for small models...\\\"\"}],\", you may ask, \",[\"$\",\"em\",null,{\"children\":\"\\\"...then why\\ndoesn't everybody train their own little guardrail and save themselves tons of money?\\\"\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"As is often the case in AI, \",[\"$\",\"strong\",null,{\"children\":\"the problem is data\"}],\". While \",[\"$\",\"em\",null,{\"children\":\"some\"}],\" of the content that guardrails\\nshould guard against are common to most users — profanity, hate speech, violence or self-harm,\\nadult content, prompt injection and jailbreak prevention... — a large portion of the guardrail's behavior\\nreally \",\"$L13\",\": some users may need guardrails against legal or medical advice,\\nothers may want to keep away from mentioning competitors, others may want to avoid political topics,\\nand so on.\"]}],\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\"]]}]\n"])</script><script>self.__next_f.push([1,"41:I[50270,[\"553\",\"static/chunks/553-2a38a743b7160be3.js\",\"159\",\"static/chunks/159-6a7d7131ac72839c.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2685f9ffee653.js\"],\"CodeSnippet\"]\n13:[\"$\",\"strong\",null,{\"children\":\"depends on each user's specific needs\"}]\n14:[\"$\",\"p\",null,{\"children\":[\"Guardrail models won't just magically know what they need to guard against: they need to be taught. However,\\nteaching them requires \",[\"$\",\"strong\",null,{\"children\":\"datasets with tens of thousands of labeled utterances\"}],\", which is something that\\nmost developers (or even companies) \",[\"$\",\"strong\",null,{\"children\":\"don't have\"}],\".\"]}]\n15:[\"$\",\"h2\",null,{\"children\":\"The solution: Artifex\"}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"p\",null,{\"children\":[\"Our \",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"Artifex library\"}],\"\\novercomes this problem by allowing you to fine-tune a small, general-purpose guardrail model — one\\nwith a few, very general instructions already in place (such as profanity and hate speech detection)\\n— \",[\"$\",\"strong\",null,{\"children\":\"without the need to provide any training dataset\"}],\". You simply describe what the guardrail\\nmodel should guard against, and Artifex will fine-tune the general-purpose model on \",[\"$\",\"strong\",null,{\"children\":\"synthetic\\ndata generated on-the-fly\"}],\" based on your requirements.\"]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"p\",null,{\"children\":\"Let's see how to do that.\"}]\n18:[\"$\",\"h2\",null,{\"children\":\"Regular chatbot implementation\"}]\n19:[\"$\",\"p\",null,{\"children\":\"Let's suppose we have a little online store, and we want to develop a chatbot that acts as a\\ncustomer assistant. The chatbot should help customers track their orders, answer inquiries on\\navailable products and so on.\"}]\n1a:[\"$\",\"p\",null,{\"children\":[\"We will be using OpenAI's GPT-4.1 API, but the same principles apply to any other LLM API. The chatbot will\\nbe implemented as a FastAPI web service, with a single \",[\"$\",\"code\",null,{\"children\":\"/chat\"}],\" endpoint that receives user messages.\"]}]\n1b:[\"$\",\"p\",null,{\"children\":\"A bare-bone implementation may look like this:\"}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"$L41\",null,{\"code\":\"\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nfrom openai import OpenAI\\n\\nclient = OpenAI(api_key=\\\"YOUR_API_KEY\\\")\\n\\napp = FastAPI()\\n\\nclass ChatRequest(BaseModel):\\n  message: str\\n\\n@app.post(\\\"/chat\\\")\\nasync def chat_endpoint(req: ChatRequest):\\n\\n  user_msg = req.message\\n\\n  system_prompt = \\\"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\\\"\\n\\n  response = client.chat.completions.create(\\n      model=\\\"gpt-4.1\\\",\\n      messages=[\\n          {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_prompt},\\n          {\\\"role\\\": \\\"user\\\", \\\"content\\\": user_msg}\\n      ]\\n  )\\n\\n  reply = response.choices[0].message.content\\n  return {\\\"reply\\\": reply}\\n\"}]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of guardrail-free model\"}]\n1e:[\"$\",\"p\",null,{\"children\":[\"As of the time of writing, the GPT-4.1 API costs $2 per 1M input tokens and $8 per 1M output tokens. Since the\\nsystem prompt we provided is 30 tokens long, if we assume that the average user message is 20 tokens long and the\\naverage chatbot response is 40 tokens long, the current chatbot implementation incurs a \",[\"$\",\"strong\",null,{\"children\":\"cost of\\n$0.42 per 1K user messages\"}],\", with an average measured \",[\"$\",\"strong\",null,{\"children\":\"latency of 1.4 seconds\"}],\". Not too bad.\"]}]\n1f:[\"$\",\"h3\",null,{\"children\":\"Adding guardrails\"}]\n20:[\"$\",\"p\",null,{\"children\":[\"But here's the catch: except for the high-level guardrails embedded into every OpenAI model, this simple\\nimplementation \",[\"$\",\"strong\",null,{\"children\":\"doesn't include any kind of guardrailing\"}],\". Our chatbot will do its best to answer user\\nqueries, even when they are out-of-scope or potentially harmful.\"]}]\n21:[\"$\",\"p\",null,{\"children\":[\"Say, for instance, that a user asks our chatbot to help him build a React component for his latest full-stack\\nweb application, asks for medical advice, or tries to get the chatbot to reveal confidential information\\nabout our online store. Such attempts are not only a potential \",[\"$\",\"strong\",null,{\"children\":\"security threat and legal liability\"}],\", but\\nwill \",[\"$\",\"strong\",null,{\"children\":\"increase the overall cost\"}],\" of our chatbot, as the model will waste tokens trying to answer out-of-scope\\nqueries.\"]}]\n22:[\"$\",\"p\",null,{\"children\":[\"We could (and should), of course, refine the system prompt by instructing the chatbot to not answer\\nany queries that are unrelated to the online store, but this \",[\"$\",\"strong\",null,{\"children\":\"simple security mechanism would be insufficient\"}],\",\\nsince system prompts can be bypassed with simple stratagems (prompt injection techniques, various versions of\\n\",[\"$\",\"em\",null,{\"children\":\"\\\"forget all previous instructions and...\\\"\"}],\"). In order to"])</script><script>self.__next_f.push([1," make it impossible for the user to bypass the\\nsecurity mechanism, any effective guardrail must be implemented as an \",[\"$\",\"strong\",null,{\"children\":\"external component\"}],\".\"]}]\n23:[\"$\",\"p\",null,{\"children\":[\"In fact, to be safe we should implement \",[\"$\",\"strong\",null,{\"children\":\"not one, but two guardrails\"}],\": one to the user query, before it is\\npassed to the chatbot (to ensure the user does not input any harmful or out-of-scope content), and one to\\nthe chatbot output, before it is displayed to the user (to ensure the chatbot doesn't output any harmful or\\nout-of-scope content).\"]}]\n24:[\"$\",\"p\",null,{\"children\":\"Let's add the two-levels guardrailing system to the our chatbot:\"}]\n42:T9d8,"])</script><script>self.__next_f.push([1,"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR_API_KEY\")\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n  message: str\n  \ndef input_guardrail(message: str):\n\n  safe_topics = [\"customer service\", \"product information\", \"order status\", \"returns\", \"shipping\"]\n  prompt = f\"You are a guardrail model that ensures the user's message is appropriate for a customer service chatbot. Only allow messages related to the following topics: \" + \", \".join(safe_topics) + \". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": prompt},\n          {\"role\": \"user\", \"content\": message}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  return reply\n\ndef output_guardrail(message: str):\n\n  unsafe_content = [\"hate speech\", \"personal attacks\", \"sensitive information\", \"inappropriate language\"]\n  prompt = f\"You are a guardrail model that ensures the chatbot's response is appropriate for a customer service context. Messages containing the following types of content are considered inappropriate: \" + \", \".join(unsafe_content) + \". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": prompt},\n          {\"role\": \"user\", \"content\": message}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  return reply\n\n@app.post(\"/chat\")\nasync def chat_endpoint(req: ChatRequest):\n  \n  user_msg = req.message\n  \n  # Input Guardrail Check\n  input_check = input_guardrail(user_msg)\n  if input_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n  \n  # Generate Chatbot Response\n  system_prompt = \"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": user_msg}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  \n  # Output Guardrail Check\n  output_check = output_guardrail(reply)\n  if output_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n  \n  return {\"reply\": reply}\n"])</script><script>self.__next_f.push([1,"25:[\"$\",\"$L41\",null,{\"code\":\"$42\"}]\n26:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of guardrailed model\"}]\n27:[\"$\",\"p\",null,{\"children\":[\"Boom, our chatbot is now much safer, but its \",[\"$\",\"strong\",null,{\"children\":\"cost has increased dramatically\"}],\". For every user message, we\\nare making not one, but three separate requests to OpenAI's API: one for the input guardrail, one for chat\\ncompletion, one for the output guardrail.\"]}]\n28:[\"$\",\"p\",null,{\"children\":[\"The price increase is not even the only issue here: \",[\"$\",\"strong\",null,{\"children\":\"latency has gone up significantly\"}],\" too. Except for cases\\nin which the conversation stops at the input guardrail (if the user query is unsafe, that particular\\nconversation stops there and never makes it to the chat completion or output guardrail), all user messages now go\\nthrough three separate round-trips to the OpenAI servers instead of just one.\"]}]\n"])</script><script>self.__next_f.push([1,"29:[\"$\",\"p\",null,{\"children\":[\"To put things into perspective, this new implementation consumes, on average, an additional 130 input tokens\\nand 2 output tokens per user message. Assuming the same average number of tokens per user message as before, this\\ntranslates into an overall cost of $0.7 per 1K user messages. Since the cost of the guardrail-free chatbot\\nwas $0.42 per 1K user messages, the guardrailed chatbot is around \",[\"$\",\"strong\",null,{\"children\":\"67% more expensive\"}],\". Things don't\\nlook better latency-wise: the new average measured latency is 3.4 seconds, which means our\\nguardrailed chatbot is \",[\"$\",\"strong\",null,{\"children\":\"2.5x slower\"}],\" than the guardrai-free version.\"]}]\n"])</script><script>self.__next_f.push([1,"2a:[\"$\",\"$L12\",null,{\"src\":\"/images/blog/guardrail-cost-chart.png\",\"alt\":\"Guardrail cost chart\",\"width\":0,\"height\":0,\"style\":{\"width\":\"100%\",\"height\":\"auto\"},\"unoptimized\":true}]\n"])</script><script>self.__next_f.push([1,"2b:[\"$\",\"p\",null,{\"children\":[\"To be fair, since our guardrail's system prompt is very short (around 30 tokens, while real-world system prompts\\nare usually many times longer), our estimate on cost and latency increase is a bit of a \",[\"$\",\"strong\",null,{\"children\":\"worst-case scenario\"}],\".\\nAs the number of tokens in the system prompt rises relative to the number of tokens in the guardrail prompt,\\nthe cost and latency increase will be less pronounced. Based on our experiments with various system and guardrail\\nprompts, we found that adding guardrails increments cost and latency by \",[\"$\",\"strong\",null,{\"children\":\"anywhere between 35% and 70%\"}],\" depending\\non the number of tokens used.\"]}]\n"])</script><script>self.__next_f.push([1,"2c:[\"$\",\"h2\",null,{\"children\":\"Offloading guardrail tasks to a local model with Artifex\"}]\n2d:[\"$\",\"p\",null,{\"children\":[\"We now come to the main point of this tutorial. If only we had a way to offload guardrail-related queries to a\\nsmall guardrail model that runs locally, instead of sending them to OpenAI's API, we could reduce the number of\\nAPI calls by two-thirds, thereby \",[\"$\",\"strong\",null,{\"children\":\"cutting costs and latency\"}],\" by a significant amount.\"]}]\n2e:[\"$\",\"p\",null,{\"children\":[\"Such a solution is, in fact, possible with \",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"Artifex\"}],\".\\nArtifex allows us to create a small guardrail model based on our requirements in under five minutes and without\\nany training dataset.\"]}]\n2f:[\"$\",\"p\",null,{\"children\":\"We will again create two separate guardrails, one for the input message and one for the output message, then\\nreplace the API-based guardrails with the ones running locally.\"}]\n30:[\"$\",\"p\",null,{\"children\":\"First of all, install the Artifex library with\"}]\n31:[\"$\",\"$L41\",null,{\"code\":\"pip install artifex\"}]\n32:[\"$\",\"p\",null,{\"children\":\"Creating a guardrail model with Artifex is as simple as the following:\"}]\n33:[\"$\",\"$L41\",null,{\"code\":\"\\nfrom artifex import Artifex\\n\\ngr = Artifex().guardrail\\n\\nmodel_output_path = \\\"./input_guardrail/\\\"\\n\\ngr.train(\\n  instructions=[\\n      \\\"Queries related to customer service, product information, order status, returns and shipping are allowed.\\\",\\n      \\\"Everything else is not allowed.\\\"\\n  ],\\n  output_path=model_output_path\\n)\\n\"}]\n34:[\"$\",\"p\",null,{\"children\":[\"There. All we have done is passing to the \",[\"$\",\"code\",null,{\"children\":\"instructions\"}],\" argument of \",[\"$\",\"code\",null,{\"children\":\"Artifex().guardrail.train()\"}],\" the list of\\nallowed and not allowed queries. The training process will take a few minutes. Once it's done, the model will\\nbe saved to the \",[\"$\",\"code\",null,{\"children\":\"./input_guardrail/\"}],\" folder, together with the "])</script><script>self.__next_f.push([1,"synthetic dataset that was generated on-the-fly\\nduring training, based on the instructions we provided.\"]}]\n35:[\"$\",\"p\",null,{\"children\":\"Once the input guardrail model is ready, we can create the output guardrail model in a similar way:\"}]\n36:[\"$\",\"$L41\",null,{\"code\":\"\\nfrom artifex import Artifex\\n\\ngr = Artifex().guardrail\\n\\nmodel_output_path = \\\"./output_guardrail/\\\"\\n\\ngr.train(\\n  instructions=[\\n      \\\"Responses that contain hate speech, personal attacks, sensitive information or inappropriate language are not allowed.\\\",\\n      \\\"Everything else is allowed.\\\"\\n  ],\\n  output_path=model_output_path\\n)\\n\"}]\n37:[\"$\",\"p\",null,{\"children\":\"Now that we have both guardrail models ready, we can modify our FastAPI chatbot implementation to use the local\\nmodels instead of the API-based ones:\"}]\n43:T51c,"])</script><script>self.__next_f.push([1,"\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom artifex import Artifex\n\nclient = OpenAI(api_key=\"YOUR_API_KEY\")\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n  message: str\n\ninput_guardrail = Artifex().guardrail\noutput_guardrail = Artifex().guardrail\n\ninput_guardrail.load(\"./input_guardrail\")\noutput_guardrail.load(\"./output_guardrail\")\n\n@app.post(\"/chat\")\nasync def chat_endpoint(req: ChatRequest):\n\n  user_msg = req.message\n\n  # Input Guardrail Check\n  input_check = input_guardrail(user_msg)[0].label\n  if input_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n\n  # Generate Chatbot Response\n  system_prompt = \"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": user_msg}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n\n  # Output Guardrail Check\n  output_check = output_guardrail(reply)[0].label\n  if output_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n\n  return {\"reply\": reply}\n"])</script><script>self.__next_f.push([1,"38:[\"$\",\"$L41\",null,{\"code\":\"$43\"}]\n"])</script><script>self.__next_f.push([1,"39:[\"$\",\"p\",null,{\"children\":[\"Our code is even simpler than before. We just instantiate two guardrail models with\\n\",[\"$\",\"code\",null,{\"children\":\"Artifex().guardrail\"}],\", then load the two fine-tuned guardrails we created earlier with the \",[\"$\",\"code\",null,{\"children\":\"load()\"}],\"\\nmethod. We can then remove the two \",[\"$\",\"code\",null,{\"children\":\"input_guardrail()\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"output_guardrail()\"}],\" functions entirely,\\nreplacing them with calls to our newly loaded input and output guardrails. For more information on\\nhow to train, load and perform inference with guardrail models with Artifex, check out\\nthe \",[\"$\",\"a\",null,{\"href\":\"https://docs.tanaos.com/artifex/guardrail/train/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"\\nArtifex documentation\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"3a:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of model that uses local guardrails\"}]\n3b:[\"$\",\"p\",null,{\"children\":[\"Since all guardrail-related inference is now happening entirely on our CPU, OpenAI will not bill\\nus for it. Our chatbot cost is therefore back to its original level (prior to adding the API-based\\nguardrails), which is $0.42 per 1K user messages, while its average measured latency is 1.6 seconds.\\nThis translates to a \",[\"$\",\"strong\",null,{\"children\":\"43% reduction in costs and a 53% reduction in latency\"}],\" compared to the\\nchatbot that uses API-based guardrails.\"]}]\n3c:[\"$\",\"$L12\",null,{\"src\":\"/images/blog/local-guardrail-cost-chart.png\",\"alt\":\"Guardrail cost chart\",\"width\":0,\"height\":0,\"style\":{\"width\":\"100%\",\"height\":\"auto\"},\"unoptimized\":true,\"className\":\"mb-5\"}]\n3d:[\"$\",\"h2\",null,{\"children\":\"Wrapping up\"}]\n3e:[\"$\",\"p\",null,{\"children\":[\"In this tutorial, we have seen how to use Artifex to create small, task-specific guardrail\\nmodels that run locally on our CPU, thereby \",[\"$\",\"strong\",null,{\"children\":\"drastically cutting chatbot costs and latency\"}],\" by\\noffloading guardrail-related queries to the local models.\"]}]\n3f:[\"$\",\"p\",null,{\"children\":\"This approach is not limited to guardrail tasks only: any kind of specialized text\\nclassification task that would otherwise require expensive API calls can be offloaded to a small,\\nlocally running model created with Artifex, thereby reducing costs and latency across the board.\"}]\n40:[\"$\",\"p\",null,{\"children\":[\"If you want to learn more about Artifex and how to use it, check out our\\n\",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"GitHub repository\"}],\" and\\n\",[\"$\",\"a\",null,{\"href\":\"$undefined\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"documentation\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"b:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Cut your chatbot costs and latency by 40% using local Guardrail models. | Tanaos Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"keywords\",\"content\":\"task-specific LLM,offline NLP,text classification,without training data,NLP,Artifex,0.4.0\"}],[\"$\",\"link\",\"2\",{\"rel\":\"canonical\",\"href\":\"undefined/blog/cut-guardrail-costs/\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Cut your chatbot costs and latency by 40% using local Guardrail models.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"undefined/blog/cut-guardrail-costs/\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:image\",\"content\":\"https://tanaos.com/images/blog/cut-guardrail-costs.png\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image:alt\",\"content\":\"Cut your chatbot costs and latency by 40% using local Guardrail models.\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Cut your chatbot costs and latency by 40% using local Guardrail models.\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://tanaos.com/images/blog/cut-guardrail-costs.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"10:\"$b:metadata\"\n"])</script></body></html>