<!DOCTYPE html><!--QS4LJzPydrME_MbGtxG8l--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/0484562807a97172-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/8888a3826f4a3af4-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/eafabf029ad39a43-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/91a9a78e8a0a21a2.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5388290d976a8ef2.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c334275b0572c852.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/0fb36d5ac05ca721.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5d643ef3b3193cbd.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-6331cc938624b6b7.js"/><script src="/_next/static/chunks/4bd1b696-409494caf8c83275.js" async=""></script><script src="/_next/static/chunks/255-8db1c35057a14be6.js" async=""></script><script src="/_next/static/chunks/main-app-4ffd0bfd209b03a4.js" async=""></script><script src="/_next/static/chunks/0e762574-e88fb787cb8c3f9d.js" async=""></script><script src="/_next/static/chunks/619-28045d4483afb7ae.js" async=""></script><script src="/_next/static/chunks/356-a1921fb11ab00663.js" async=""></script><script src="/_next/static/chunks/app/layout-9ba58e19211c8ffe.js" async=""></script><script src="/_next/static/chunks/946-931eea09560c6eb2.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-9e771831df8a4887.js" async=""></script><meta name="next-size-adjust" content=""/><title>Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail. | Tanaos Blog</title><meta name="keywords" content="task-specific LLM,offline NLP,text classification,without training data,NLP,Artifex,0.4.0"/><link rel="canonical" href="undefined/blog/cut-guardrail-costs/"/><meta property="og:title" content="Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."/><meta property="og:url" content="undefined/blog/cut-guardrail-costs/"/><meta property="og:image" content="https://tanaos.com/images/blog/cut-guardrail-costs.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail."/><meta name="twitter:image" content="https://tanaos.com/images/blog/cut-guardrail-costs.png"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_cc80f9"><div hidden=""><!--$--><!--/$--></div><div class="app"><div class="row Navbar_navbar__3CvTR m-0 false"><div class="m-0 p-0 Navbar_navbar-large-devices__rsA5K"><div class="col m-0 p-0 text-start"><img alt="Create task-specific LLMs for NLP and Text Classification | Tanaos" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="Navbar_logo__RSROF" style="color:transparent" src="/images/logo.png"/></div><div class="col m-0 p-0 text-end d-flex align-items-center justify-content-end Navbar_navbar-actions__PC49b"><nav class="Navigation_navigation__Eln2g"><a href="https://huggingface.co/tanaos" rel="noreferrer" target="_blank">Public models ðŸ¤—</a><a href="/models">Custom Models</a><a href="/blog/">Blog</a><a class="btn btn-white" href="#try-it-out">Create your model <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="ms-1" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M1 8a.5.5 0 0 1 .5-.5h11.793l-3.147-3.146a.5.5 0 0 1 .708-.708l4 4a.5.5 0 0 1 0 .708l-4 4a.5.5 0 0 1-.708-.708L13.293 8.5H1.5A.5.5 0 0 1 1 8"></path></svg></a></nav></div></div><div class="Navbar_navbar-small-devices__w8Bl9 d-md-none m-0 p-0 d-flex align-items-center"><div class="col-10 m-0 p-0 text-start"><img alt="Create task-specific LLMs for NLP and Text Classification | Tanaos" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="Navbar_logo__RSROF" style="color:transparent" src="/images/logo.png"/></div><div class="col-2 m-0 p-0 text-end"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="Navbar_navbar-toggle-icon__TXjJe" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M2.5 12a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5m0-4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 0 1H3a.5.5 0 0 1-.5-.5"></path></svg></div></div><div class="d-md-none Navbar_navbar-collapse__09PzW text-start false"><div class="mt-4"><nav class="Navigation_navigation__Eln2g"><a href="https://huggingface.co/tanaos" rel="noreferrer" target="_blank">Public models ðŸ¤—</a><a href="/models">Custom Models</a><a href="/blog/">Blog</a><a class="btn btn-white" href="#try-it-out">Create your model <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="ms-1" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M1 8a.5.5 0 0 1 .5-.5h11.793l-3.147-3.146a.5.5 0 0 1 .708-.708l4 4a.5.5 0 0 1 0 .708l-4 4a.5.5 0 0 1-.708-.708L13.293 8.5H1.5A.5.5 0 0 1 1 8"></path></svg></a></nav></div></div></div><main class="content"><article class="BlogPage_article__fv9gc"><a class="BlogPage_back-link__G6xqk" href="/blog/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M15 8a.5.5 0 0 0-.5-.5H2.707l3.147-3.146a.5.5 0 1 0-.708-.708l-4 4a.5.5 0 0 0 0 .708l4 4a.5.5 0 0 0 .708-.708L2.707 8.5H14.5A.5.5 0 0 0 15 8"></path></svg> Back to blog</a><h1 class="mt-4">Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.</h1><p class="BlogPage_subtitle__wqaGw">Reduce your reliance on expensive API calls by offloading guardrail-specific queries to self-hosted, small guardrail models that don&#x27;t need a GPU.</p><p class="BlogPage_date__VyN7U">December 12, 2025</p><img alt="Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail." loading="lazy" width="1200" height="630" decoding="async" data-nimg="1" class="mt-5 mb-5 BlogPage_post-image__Du0zR" style="color:transparent" src="/images/blog/cut-guardrail-costs.png"/><p>It may or may not surprise you that, in most chatbots implemented through an LLM API, guardrail-related queries
account on average for <strong>40% of total API costs</strong>. And by the way, if you need a refresher on what
guardrails are and why they are important, check out our previous
<a href="/blog/guardrail-models/">post on guardrails</a>.</p>
<p>Despite their high cost, guardrailing tasks are straightforward and easy to get right even for small models
in the 0.1B parameters range (or even smaller). Making a call to an LLM API just to check whether a message is
safe is effectively <strong>a waste of money</strong>.</p>
<p>The problem is all the more frustrating when you consider that any respectable chatbot should make <strong>not one,
but two</strong> guardrail-related API calls for every user query: one to make sure the user query is safe, before
it is fed to the chatbot, and the other to make sure that the chatbot output is safe, before showing it
to the user.</p>
<p>Latency is another important factor to consider: every additional API call adds network latency by increasing
the number of round-trips to the API server.</p>
<h2>So why don&#x27;t people self-host their guardrail model?</h2>
<p><em>&quot;If guardrailing tasks are so easy to get right even for small models...&quot;</em>, you may ask, <em>&quot;...then why
doesn&#x27;t everybody train their own guardrail, self-host it on the same server as the chatbot backend
and save themselves tons of money?&quot;</em>.</p>
<p>As is often the case in AI, <strong>the problem is data</strong>. While <em>some</em> of the content that guardrails
should guard against are common to most users (profanity, hate speech, violence or self-harm,
adult content, prompt injection and jailbreak prevention...) a large portion of the guardrail&#x27;s behavior
really <strong>depends on each user&#x27;s specific needs</strong>: some users may need guardrails against legal or medical advice,
others may want to keep away from mentioning competitors, others may want to avoid political topics,
and so on.</p>
<p>Guardrail models won&#x27;t just magically know what they need to guard against: they need to be taught. However,
teaching them requires <strong>datasets with tens of thousands of labeled utterances</strong>, which is something that
most developers (or even companies) <strong>don&#x27;t have</strong>.</p>
<h2>Regular chatbot implementation</h2>
<p>Let&#x27;s suppose we have a little online store, and we want to develop a chatbot that acts as a
customer assistant. The chatbot should help customers track their orders, answer inquiries on
available products and so on.</p>
<p>Since most developers would implement a guardrailed chatbot entirely through an LLM API, that&#x27;s the
scenario we will be using as our starting point. We will employ <strong>OpenAI&#x27;s GPT-4.1 API</strong>, but the
same principles apply to any other LLM API. The chatbot will be implemented as a <strong>FastAPI web
service</strong>, with a single <code>/chat</code> endpoint that receives user messages.</p>
<p>A bare-bone implementation may look like this:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>
<span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<h3>Cost &amp; latency assessment of guardrail-free chatbot</h3>
<p>As of the time of writing, the GPT-4.1 API costs $2 per 1M input tokens and $8 per 1M output tokens. Since the
system prompt we provided is 30 tokens long, if we assume that the average user message is 20 tokens long and the
average chatbot response is 40 tokens long, the current chatbot implementation incurs a <strong>cost of
$0.42 per 1K user messages</strong>, with an average measured <strong>latency of 1.4 seconds</strong>. Not too bad.</p>
<h3>Adding guardrails</h3>
<p>But here&#x27;s the catch: except for the high-level guardrails embedded into every OpenAI model, this simple
implementation <strong>doesn&#x27;t include any kind of guardrailing</strong>. Our chatbot will do its best to answer user
queries, even when they are out-of-scope or potentially harmful.</p>
<p>Say, for instance, that a user asks our chatbot to help him build a React component for his latest full-stack
web application, asks for medical advice, or tries to get the chatbot to reveal confidential information
about our online store. Such attempts are not only a potential <strong>security threat and legal liability</strong>, but
will <strong>increase the overall cost</strong> of our chatbot, as the model will waste tokens trying to answer out-of-scope
queries.</p>
<p>We could (and should), of course, refine the system prompt by instructing the chatbot to not answer
any queries that are unrelated to the online store, but this <strong>naive security mechanism would be insufficient</strong>,
since system prompts can be bypassed with simple stratagems (prompt injection techniques, various versions of
<em>&quot;forget all previous instructions and...&quot;</em>). In order to make it impossible the users to bypass the
security mechanism, any effective guardrail must be implemented as an <strong>external component</strong>.</p>
<p>In fact, to be safe we should implement <strong>not one, but two guardrails</strong>: one for the user query, before it is
passed to the chatbot (to ensure the user does not input any harmful or out-of-scope content), and one for
the chatbot output, before it is displayed to the user (to ensure the chatbot doesn&#x27;t output any harmful or
out-of-scope content).</p>
<p>Let&#x27;s add the two-levels guardrailing system to the our chatbot:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>  
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  safe_topics </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;customer service&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;product information&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;order status&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;returns&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;shipping&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token string-interpolation" style="color:hsl(95, 38%, 62%)">f&quot;You are a guardrail model that ensures the user&#x27;s message is appropriate for a customer service chatbot. Only allow messages related to the following topics: &quot;</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;, &quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>join</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>safe_topics</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;. If the message is inappropriate, respond with &#x27;unsafe&#x27;. Otherwise, respond with &#x27;safe&#x27;.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> message</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> reply
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  unsafe_content </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;hate speech&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;personal attacks&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;sensitive information&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;inappropriate language&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token string-interpolation" style="color:hsl(95, 38%, 62%)">f&quot;You are a guardrail model that ensures the chatbot&#x27;s response is appropriate for a customer service context. Messages containing the following types of content are considered inappropriate: &quot;</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;, &quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>join</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>unsafe_content</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">+</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;. If the message is inappropriate, respond with &#x27;unsafe&#x27;. Otherwise, respond with &#x27;safe&#x27;.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> message</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> reply
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>  
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>  
<span>  </span><span class="token" style="color:#aaa"># Input Guardrail Check</span><span>
</span><span>  input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>  
<span>  </span><span class="token" style="color:#aaa"># Generate Chatbot Response</span><span>
</span><span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span>  
<span>  </span><span class="token" style="color:#aaa"># Output Guardrail Check</span><span>
</span><span>  output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>reply</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>  
<span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<h3>Cost &amp; latency assessment of guardrailed chatbot</h3>
<p>Boom, our chatbot is now much safer, but its <strong>cost has increased dramatically</strong>. For every user message, we
are now making not one, but three separate requests to OpenAI&#x27;s API: one for the input guardrail, one for chat
completion, one for the output guardrail.</p>
<p>The price increase is not even the only issue here: <strong>latency has gone up significantly</strong> too. Except for cases
in which the conversation stops at the input guardrail (if the user query is unsafe, that particular
conversation stops there and never makes it to the chat completion or output guardrail), all user messages now go
through three separate round-trips to the OpenAI servers instead of just one.</p>
<p>To put things into perspective, this new implementation consumes, on average, an additional 130 input tokens
and 2 output tokens per user message. Assuming the same average number of tokens per user message as before, this
translates into an overall cost of $0.7 per 1K user messages. Since the cost of the guardrail-free chatbot
was $0.42 per 1K user messages, the guardrailed chatbot is around <strong>67% more expensive</strong>. Things don&#x27;t
look better latency-wise: the new average measured latency is 3.4 seconds, which means our
guardrailed chatbot is <strong>2.5x slower</strong> than the guardrai-free version.</p>
<img alt="Guardrail cost chart" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="mb-5" style="color:transparent;width:100%;height:auto" src="/images/blog/guardrail-cost-chart.png"/>
<h2>Offloading guardrail tasks to a self-hosted model with Artifex</h2>
<p>We now come to the main point of this tutorial. If only we had a way to offload guardrail-related
queries to a small guardrail model that runs on the same server as the chatbot backend, instead of
sending them to OpenAI&#x27;s API, we could reduce the number of API calls by two-thirds,
thereby <strong>cutting costs and latency</strong> by a significant amount.</p>
<p>Such a solution is, in fact, possible with <a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">Artifex</a>.
Artifex is an open-source Python library for using and fine-tuning small, task-specific LLMs without the need for labeled
data or GPUs.</p>
<p>We will again create two separate guardrails, one for the input message and one for the output message, then
replace the API-based guardrails with the ones running locally.</p>
<p>First of all, install the Artifex library with</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>pip install artifex</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Creating a guardrail model with Artifex is as simple as the following:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>gr </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>model_output_path </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./input_guardrail/&quot;</span><span>
</span>
<span>gr</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>train</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>  instructions</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Queries related to customer service, product information, order status, returns and shipping are allowed.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Everything else is not allowed.&quot;</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>  output_path</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span>model_output_path
</span><span></span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>There. All we have done is passing to the <code>instructions</code> argument of <code>Artifex().guardrail.train()</code> the list of
allowed and not allowed queries. Read the two strings inside the list passed as the <code>instructions</code>
argument: they describe the same allowed and unallowed content that we passed to the
API-based guardrail&#x27;s system prompt earlier.</p>
<p>The training process will take a few minutes. Once it&#x27;s done, the model will
be saved to the <code>./input_guardrail/</code> folder, together with the synthetic dataset that was generated on-the-fly
during training, based on the instructions we provided.</p>
<p>Once the input guardrail model is ready, we can create the output guardrail model in a similar way:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>gr </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>model_output_path </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./output_guardrail/&quot;</span><span>
</span>
<span>gr</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>train</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>  instructions</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Responses that contain hate speech, personal attacks, sensitive information or inappropriate language are not allowed.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;Everything else is allowed.&quot;</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>  output_path</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span>model_output_path
</span><span></span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Now that both guardrail models are ready, we can modify our FastAPI chatbot implementation to use
the guardrail models we just created instead of the API-based ones:</p>
<div class="CodeSnippet_code-snippet-container__pp8Le mb-5"><pre class="code-block" style="background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1rem;margin:0;overflow:auto;border-radius:0.75rem"><code class="language-python" style="white-space:pre;background:hsl(220, 13%, 18%);color:hsl(220, 14%, 71%);text-shadow:0 1px rgba(0, 0, 0, 0.3);font-family:&quot;Fira Code&quot;, &quot;Fira Mono&quot;, Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;direction:ltr;text-align:left;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> fastapi </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> FastAPI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> pydantic </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> BaseModel
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> openai </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> OpenAI
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">from</span><span> artifex </span><span class="token" style="color:hsl(286, 60%, 67%)">import</span><span> Artifex
</span>
<span>client </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> OpenAI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>api_key</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;YOUR_API_KEY&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>app </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> FastAPI</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token" style="color:hsl(286, 60%, 67%)">class</span><span> </span><span class="token" style="color:hsl(29, 54%, 61%)">ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>BaseModel</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>  message</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">str</span><span>
</span>
<span>input_guardrail </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span><span>output_guardrail </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> Artifex</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>guardrail
</span>
<span>input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>load</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./input_guardrail&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span>output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>load</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;./output_guardrail&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span></span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">@app</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">.</span><span class="token decorator annotation" style="color:hsl(220, 14%, 71%)">post</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;/chat&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span><span></span><span class="token" style="color:hsl(286, 60%, 67%)">async</span><span> </span><span class="token" style="color:hsl(286, 60%, 67%)">def</span><span> </span><span class="token" style="color:hsl(207, 82%, 66%)">chat_endpoint</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>req</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> ChatRequest</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span>
<span>  user_msg </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> req</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message
</span>
<span>  </span><span class="token" style="color:#aaa"># Input Guardrail Check</span><span>
</span><span>  input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> input_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>label
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> input_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>
<span>  </span><span class="token" style="color:#aaa"># Generate Chatbot Response</span><span>
</span><span>  system_prompt </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.&quot;</span><span>
</span>
<span>  response </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> client</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>chat</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>completions</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>create</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>
</span><span>      model</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;gpt-4.1&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>      messages</span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;system&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> system_prompt</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span>
</span><span>          </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;role&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;user&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">,</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;content&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> user_msg</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span><span>      </span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span>
</span><span>  </span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span>
</span>
<span>  reply </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> response</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>choices</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>message</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>content
</span>
<span>  </span><span class="token" style="color:#aaa"># Output Guardrail Check</span><span>
</span><span>  output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">=</span><span> output_guardrail</span><span class="token" style="color:hsl(220, 14%, 71%)">(</span><span>reply</span><span class="token" style="color:hsl(220, 14%, 71%)">)</span><span class="token" style="color:hsl(220, 14%, 71%)">[</span><span class="token" style="color:hsl(29, 54%, 61%)">0</span><span class="token" style="color:hsl(220, 14%, 71%)">]</span><span class="token" style="color:hsl(220, 14%, 71%)">.</span><span>label
</span><span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">if</span><span> output_check </span><span class="token" style="color:hsl(207, 82%, 66%)">==</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;unsafe&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span>
</span><span>      </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> </span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;I&#x27;m sorry, but I cannot help you with that.&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span>
<span>  </span><span class="token" style="color:hsl(286, 60%, 67%)">return</span><span> </span><span class="token" style="color:hsl(220, 14%, 71%)">{</span><span class="token" style="color:hsl(95, 38%, 62%)">&quot;reply&quot;</span><span class="token" style="color:hsl(220, 14%, 71%)">:</span><span> reply</span><span class="token" style="color:hsl(220, 14%, 71%)">}</span><span>
</span></code></pre><button class=""><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1z"></path><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0z"></path></svg></button></div>
<p>Our code is even simpler than before. We just instantiate two guardrail models with
<code>Artifex().guardrail</code>, then load the two fine-tuned guardrails we created earlier with the <code>load()</code>
method. We can then remove the two <code>input_guardrail()</code> and <code>output_guardrail()</code> functions entirely,
replacing them with calls to our newly loaded input and output guardrails. The rest is exactly
the same as before.</p>
<p>For more information on how to train, load and perform inference with guardrail models with Artifex, check out
the <a href="https://docs.tanaos.com/artifex/guardrail/train/" target="_blank" rel="noreferrer">
Artifex documentation</a>.</p>
<h3>Cost &amp; latency assessment of chatbot that uses self-hosted guardrails</h3>
<p>Since all guardrail-related inference is now happening entirely on our CPU, OpenAI will not bill
us for it. Our chatbot cost is therefore back to its original level (prior to adding the API-based
guardrails), which is $0.42 per 1K user messages, while its average measured latency is 1.6 seconds.
This translates to a <strong>43% reduction in costs and a 53% reduction in latency</strong> compared to the
chatbot that uses API-based guardrails.</p>
<img alt="Guardrail cost chart" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="mb-5" style="color:transparent;width:100%;height:auto" src="/images/blog/local-guardrail-cost-chart.png"/>
<h2>Wrapping up</h2>
<p>In this tutorial, we have seen how to use Artifex to create small, task-specific guardrail
models that run on the same server as the chatbot backend, thereby <strong>cutting chatbot costs by
43% and latency by 53%</strong>, by offloading guardrail-related queries to the self-hosted models.</p>
<p>This approach is not limited to guardrail tasks only: any kind of specialized text
classification task that would otherwise require expensive API calls can be offloaded to a small,
locally running model created with Artifex, thereby reducing costs and latency across the board.</p>
<p>If you want to learn more about Artifex and how to use it, check out its
<a href="https://github.com/tanaos/artifex" target="_blank" rel="noreferrer">GitHub repository</a> and
<a target="_blank" rel="noreferrer">documentation</a>.</p></article><!--$--><!--/$--></main><footer class="Footer_footer__Ir1kR"><div class="Footer_footer-inner__mvqEE"><div class="Footer_footer-top__kBSno"><div class="Footer_footer-nav-columns__gDMDt"><div class="Footer_footer-nav-column__RzaIM"><div class="Footer_footer-nav-title__Xx_sD">Main</div><a href="/">Home</a><a href="/blog/">Blog</a><a href="https://platform.tanaos.com" rel="noreferrer" target="_blank">Platform</a></div><div class="Footer_footer-nav-column__RzaIM"><div class="Footer_footer-nav-title__Xx_sD">Models</div><a href="/models/ticket-classification/">Ticket Classification</a><a href="/models/contact-form-spam-filter/">Contact Form Spam Filter</a><a href="/models/email-intent-detection/">Email Intent Detection</a><a href="/models/chatbot-safety-moderation/">Chatbot Safety &amp; Moderation</a><a href="/models/blog-posts-moderation/">Blog Posts Moderation</a><a href="/models/anomaly-fraud-detection/">Anomaly &amp; Fraud Detection</a><a href="/models/predictive-maintenance/">Predictive Maintenance</a></div><div class="Footer_footer-nav-column__RzaIM"><div class="Footer_footer-nav-title__Xx_sD">Socials</div><a href="https://github.com/tanaos" rel="noreferrer" target="_blank" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" style="vertical-align:middle;margin-right:8px" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"></path></svg> GitHub</a><a href="https://huggingface.co/tanaos" rel="noreferrer" target="_blank" aria-label="Hugging Face"><span style="font-size:1.2em;vertical-align:middle;margin-right:8px">ðŸ¤—</span> Hugging Face</a><a href="https://www.linkedin.com/company/tanaos" rel="noreferrer" target="_blank" aria-label="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" style="vertical-align:middle;margin-right:8px" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg> LinkedIn</a></div></div></div><div class="Footer_footer-divider__t_9Q_"></div><div class="Footer_footer-bottom__3DnO7"><span>Â© <!-- -->2026<!-- --> Tanaos. All rights reserved.</span></div></div></footer></div><script src="/_next/static/chunks/webpack-6331cc938624b6b7.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[58665,[\"87\",\"static/chunks/0e762574-e88fb787cb8c3f9d.js\",\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"356\",\"static/chunks/356-a1921fb11ab00663.js\",\"177\",\"static/chunks/app/layout-9ba58e19211c8ffe.js\"],\"Navbar\"]\n3:I[9766,[],\"\"]\n4:I[98924,[],\"\"]\n5:I[52619,[\"87\",\"static/chunks/0e762574-e88fb787cb8c3f9d.js\",\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"946\",\"static/chunks/946-931eea09560c6eb2.js\",\"356\",\"static/chunks/356-a1921fb11ab00663.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-9e771831df8a4887.js\"],\"\"]\nf:I[57150,[],\"\"]\n:HL[\"/_next/static/media/0484562807a97172-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/8888a3826f4a3af4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/eafabf029ad39a43-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/91a9a78e8a0a21a2.css\",\"style\"]\n:HL[\"/_next/static/css/5388290d976a8ef2.css\",\"style\"]\n:HL[\"/_next/static/css/c334275b0572c852.css\",\"style\"]\n:HL[\"/_next/static/css/0fb36d5ac05ca721.css\",\"style\"]\n:HL[\"/_next/static/css/5d643ef3b3193cbd.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"QS4LJzPydrME-MbGtxG8l\",\"p\":\"\",\"c\":[\"\",\"blog\",\"cut-guardrail-costs\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"cut-guardrail-costs\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/91a9a78e8a0a21a2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5388290d976a8ef2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c334275b0572c852.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_cc80f9\",\"children\":[[\"$\",\"div\",null,{\"className\":\"app\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"className\":\"content\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"Footer_footer__Ir1kR\",\"children\":[\"$\",\"div\",null,{\"className\":\"Footer_footer-inner__mvqEE\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Footer_footer-top__kBSno\",\"children\":[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-columns__gDMDt\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-column__RzaIM\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-title__Xx_sD\",\"children\":\"Main\"}],[\"$\",\"$L5\",null,{\"href\":\"/\",\"children\":\"Home\"}],[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"children\":\"Blog\"}],[\"$\",\"a\",null,{\"href\":\"https://platform.tanaos.com\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"children\":\"Platform\"}]]}],[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-column__RzaIM\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-title__Xx_sD\",\"children\":\"Models\"}],[\"$\",\"a\",null,{\"href\":\"/models/ticket-classification/\",\"children\":\"Ticket Classification\"}],[\"$\",\"a\",null,{\"href\":\"/models/contact-form-spam-filter/\",\"children\":\"Contact Form Spam Filter\"}],[\"$\",\"a\",null,{\"href\":\"/models/email-intent-detection/\",\"children\":\"Email Intent Detection\"}],[\"$\",\"a\",null,{\"href\":\"/models/chatbot-safety-moderation/\",\"children\":\"Chatbot Safety \u0026 Moderation\"}],[\"$\",\"a\",null,{\"href\":\"/models/blog-posts-moderation/\",\"children\":\"Blog Posts Moderation\"}],[\"$\",\"a\",null,{\"href\":\"/models/anomaly-fraud-detection/\",\"children\":\"Anomaly \u0026 Fraud Detection\"}],[\"$\",\"a\",null,{\"href\":\"/models/predictive-maintenance/\",\"children\":\"Predictive Maintenance\"}]]}],[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-column__RzaIM\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Footer_footer-nav-title__Xx_sD\",\"children\":\"Socials\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"aria-label\":\"GitHub\",\"children\":[[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"style\":{\"color\":\"$undefined\",\"verticalAlign\":\"middle\",\"marginRight\":8},\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8\",\"children\":[]}]]],\"className\":\"$undefined\",\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}],\" GitHub\"]}],\"$L6\",\"$L7\"]}]]}]}],\"$L8\",\"$L9\"]}]}]]}],\"$La\"]}]}]]}],{\"children\":[\"blog\",\"$Lb\",{\"children\":[[\"slug\",\"cut-guardrail-costs\",\"d\"],\"$Lc\",{\"children\":[\"__PAGE__\",\"$Ld\",{},null,false]},null,false]},null,false]},null,false],\"$Le\",false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:I[41402,[\"87\",\"static/chunks/0e762574-e88fb787cb8c3f9d.js\",\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"356\",\"static/chunks/356-a1921fb11ab00663.js\",\"177\",\"static/chunks/app/layout-9ba58e19211c8ffe.js\"],\"\"]\n12:I[24431,[],\"OutletBoundary\"]\n14:I[15278,[],\"AsyncMetadataOutlet\"]\n16:I[24431,[],\"ViewportBoundary\"]\n18:I[24431,[],\"MetadataBoundary\"]\n19:\"$Sreact.suspense\"\n6:[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/tanaos\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"aria-label\":\"Hugging Face\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"fontSize\":\"1.2em\",\"verticalAlign\":\"middle\",\"marginRight\":8},\"children\":\"ðŸ¤—\"}],\" Hugging Face\"]}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/company/tanaos\",\"rel\":\"noreferrer\",\"target\":\"_blank\",\"aria-label\":\"LinkedIn\",\"children\":[[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"style\":{\"color\":\"$undefined\",\"verticalAlign\":\"middle\",\"marginRight\":8},\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z\",\"children\":[]}]]],\"className\":\"$undefined\",\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}],\" LinkedIn\"]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"className\":\"Footer_footer-divider__t_9Q_\"}]\n9:[\"$\",\"div\",null,{\"className\":\"Footer_footer-bottom__3DnO7\",\"children\":[\"$\",\"span\",null,{\"children\":[\"Â© \",2026,\" Tanaos. All rights reserved.\"]}]}]\na:[\"$\",\"$L10\",null,{\"id\":\"ga-init\",\"strategy\":\"afterInteractive\",\"children\":\"\\n                        window.dataLayer = window.dataLayer || [];\\n                        function gtag(){dataLayer.push(arguments);}\\n                        gtag('js', new Date());\\n                        gtag('config', 'G-HRQ77GT2C8');\\n                    \"}]\nb:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\nc:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\nd:[\"$\",\"$1\",\"c\",{\"children\":[\"$L11\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/0fb36d5ac05ca721.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5d643ef3b3193cbd.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L12\",null,{\"children\":[\"$L13\",[\"$\",\"$L14\",null,{\"promise\":\"$@15\"}]]}]]}]\ne:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L16\",null,{\"children\":\"$L17\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L18\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$19\",null,{\"fallback\":null,\"children\":\"$L1a\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:I[81356,[\"87\",\"static/chunks/0e762574-e88fb787cb8c3f9d.js\",\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"946\",\"static/chunks/946-931eea09560c6eb2.js\",\"356\",\"static/chunks/356-a1921fb11ab00663.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-9e771831df8a4887.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"article\",null,{\"className\":\"BlogPage_article__fv9gc\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/\",\"className\":\"BlogPage_back-link__G6xqk\",\"children\":[[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"fillRule\":\"evenodd\",\"d\":\"M15 8a.5.5 0 0 0-.5-.5H2.707l3.147-3.146a.5.5 0 1 0-.708-.708l-4 4a.5.5 0 0 0 0 .708l4 4a.5.5 0 0 0 .708-.708L2.707 8.5H14.5A.5.5 0 0 0 15 8\",\"children\":[]}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}],\" Back to blog\"]}],[\"$\",\"h1\",null,{\"className\":\"mt-4\",\"children\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_subtitle__wqaGw\",\"children\":\"Reduce your reliance on expensive API calls by offloading guardrail-specific queries to self-hosted, small guardrail models that don't need a GPU.\"}],[\"$\",\"p\",null,{\"className\":\"BlogPage_date__VyN7U\",\"children\":\"December 12, 2025\"}],[\"$\",\"$L1b\",null,{\"className\":\"mt-5 mb-5 BlogPage_post-image__Du0zR\",\"src\":\"/images/blog/cut-guardrail-costs.png\",\"alt\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.\",\"width\":1200,\"height\":630,\"unoptimized\":true}],[[\"$\",\"p\",null,{\"children\":[\"It may or may not surprise you that, in most chatbots implemented through an LLM API, guardrail-related queries\\naccount on average for \",[\"$\",\"strong\",null,{\"children\":\"40% of total API costs\"}],\". And by the way, if you need a refresher on what\\nguardrails are and why they are important, check out our previous\\n\",[\"$\",\"$L5\",null,{\"href\":\"/blog/guardrail-models/\",\"children\":\"post on guardrails\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Despite their high cost, guardrailing tasks are straightforward and easy to get right even for small models\\nin the 0.1B parameters range (or even smaller). Making a call to an LLM API just to check whether a message is\\nsafe is effectively \",[\"$\",\"strong\",null,{\"children\":\"a waste of money\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The problem is all the more frustrating when you consider that any respectable chatbot should make \",[\"$\",\"strong\",null,{\"children\":\"not one,\\nbut two\"}],\" guardrail-related API calls for every user query: one to make sure the user query is safe, before\\nit is fed to the chatbot, and the other to make sure that the chatbot output is safe, before showing it\\nto the user.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Latency is another important factor to consider: every additional API call adds network latency by increasing\\nthe number of round-trips to the API server.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"So why don't people self-host their guardrail model?\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"\\\"If guardrailing tasks are so easy to get right even for small models...\\\"\"}],\", you may ask, \",[\"$\",\"em\",null,{\"children\":\"\\\"...then why\\ndoesn't everybody train their own guardrail, self-host it on the same server as the chatbot backend\\nand save themselves tons of money?\\\"\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"As is often the case in AI, \",[\"$\",\"strong\",null,{\"children\":\"the problem is data\"}],\". While \",[\"$\",\"em\",null,{\"children\":\"some\"}],\" of the content that guardrails\\nshould guard against are common to most users (profanity, hate speech, violence or self-harm,\\nadult content, prompt injection and jailbreak prevention...) a large portion of the guardrail's behavior\\nreally \",[\"$\",\"strong\",null,{\"children\":\"depends on each user's specific needs\"}],\": some users may need guardrails against legal or medical advice,\\nothers may want to keep away from mentioning competitors, others may want to avoid political topics,\\nand so on.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Guardrail models won't just magically know what they need to guard against: they need to be taught. However,\\nteaching them requires \",\"$L1c\",\", which is something that\\nmost developers (or even companies) \",\"$L1d\",\".\"]}],\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\"]]}]\n"])</script><script>self.__next_f.push([1,"48:I[50270,[\"87\",\"static/chunks/0e762574-e88fb787cb8c3f9d.js\",\"619\",\"static/chunks/619-28045d4483afb7ae.js\",\"946\",\"static/chunks/946-931eea09560c6eb2.js\",\"356\",\"static/chunks/356-a1921fb11ab00663.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-9e771831df8a4887.js\"],\"CodeSnippet\"]\n1c:[\"$\",\"strong\",null,{\"children\":\"datasets with tens of thousands of labeled utterances\"}]\n1d:[\"$\",\"strong\",null,{\"children\":\"don't have\"}]\n1e:[\"$\",\"h2\",null,{\"children\":\"Regular chatbot implementation\"}]\n1f:[\"$\",\"p\",null,{\"children\":\"Let's suppose we have a little online store, and we want to develop a chatbot that acts as a\\ncustomer assistant. The chatbot should help customers track their orders, answer inquiries on\\navailable products and so on.\"}]\n20:[\"$\",\"p\",null,{\"children\":[\"Since most developers would implement a guardrailed chatbot entirely through an LLM API, that's the\\nscenario we will be using as our starting point. We will employ \",[\"$\",\"strong\",null,{\"children\":\"OpenAI's GPT-4.1 API\"}],\", but the\\nsame principles apply to any other LLM API. The chatbot will be implemented as a \",[\"$\",\"strong\",null,{\"children\":\"FastAPI web\\nservice\"}],\", with a single \",[\"$\",\"code\",null,{\"children\":\"/chat\"}],\" endpoint that receives user messages.\"]}]\n21:[\"$\",\"p\",null,{\"children\":\"A bare-bone implementation may look like this:\"}]\n"])</script><script>self.__next_f.push([1,"22:[\"$\",\"$L48\",null,{\"code\":\"from fastapi import FastAPI\\nfrom pydantic import BaseModel\\nfrom openai import OpenAI\\n\\nclient = OpenAI(api_key=\\\"YOUR_API_KEY\\\")\\n\\napp = FastAPI()\\n\\nclass ChatRequest(BaseModel):\\n  message: str\\n\\n@app.post(\\\"/chat\\\")\\nasync def chat_endpoint(req: ChatRequest):\\n\\n  user_msg = req.message\\n\\n  system_prompt = \\\"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\\\"\\n\\n  response = client.chat.completions.create(\\n      model=\\\"gpt-4.1\\\",\\n      messages=[\\n          {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_prompt},\\n          {\\\"role\\\": \\\"user\\\", \\\"content\\\": user_msg}\\n      ]\\n  )\\n\\n  reply = response.choices[0].message.content\\n  return {\\\"reply\\\": reply}\\n\"}]\n"])</script><script>self.__next_f.push([1,"23:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of guardrail-free chatbot\"}]\n24:[\"$\",\"p\",null,{\"children\":[\"As of the time of writing, the GPT-4.1 API costs $2 per 1M input tokens and $8 per 1M output tokens. Since the\\nsystem prompt we provided is 30 tokens long, if we assume that the average user message is 20 tokens long and the\\naverage chatbot response is 40 tokens long, the current chatbot implementation incurs a \",[\"$\",\"strong\",null,{\"children\":\"cost of\\n$0.42 per 1K user messages\"}],\", with an average measured \",[\"$\",\"strong\",null,{\"children\":\"latency of 1.4 seconds\"}],\". Not too bad.\"]}]\n25:[\"$\",\"h3\",null,{\"children\":\"Adding guardrails\"}]\n26:[\"$\",\"p\",null,{\"children\":[\"But here's the catch: except for the high-level guardrails embedded into every OpenAI model, this simple\\nimplementation \",[\"$\",\"strong\",null,{\"children\":\"doesn't include any kind of guardrailing\"}],\". Our chatbot will do its best to answer user\\nqueries, even when they are out-of-scope or potentially harmful.\"]}]\n27:[\"$\",\"p\",null,{\"children\":[\"Say, for instance, that a user asks our chatbot to help him build a React component for his latest full-stack\\nweb application, asks for medical advice, or tries to get the chatbot to reveal confidential information\\nabout our online store. Such attempts are not only a potential \",[\"$\",\"strong\",null,{\"children\":\"security threat and legal liability\"}],\", but\\nwill \",[\"$\",\"strong\",null,{\"children\":\"increase the overall cost\"}],\" of our chatbot, as the model will waste tokens trying to answer out-of-scope\\nqueries.\"]}]\n28:[\"$\",\"p\",null,{\"children\":[\"We could (and should), of course, refine the system prompt by instructing the chatbot to not answer\\nany queries that are unrelated to the online store, but this \",[\"$\",\"strong\",null,{\"children\":\"naive security mechanism would be insufficient\"}],\",\\nsince system prompts can be bypassed with simple stratagems (prompt injection techniques, various versions of\\n\",[\"$\",\"em\",null,{\"children\":\"\\\"forget all previous instructions and...\\\"\"}],\"). In order t"])</script><script>self.__next_f.push([1,"o make it impossible the users to bypass the\\nsecurity mechanism, any effective guardrail must be implemented as an \",[\"$\",\"strong\",null,{\"children\":\"external component\"}],\".\"]}]\n29:[\"$\",\"p\",null,{\"children\":[\"In fact, to be safe we should implement \",[\"$\",\"strong\",null,{\"children\":\"not one, but two guardrails\"}],\": one for the user query, before it is\\npassed to the chatbot (to ensure the user does not input any harmful or out-of-scope content), and one for\\nthe chatbot output, before it is displayed to the user (to ensure the chatbot doesn't output any harmful or\\nout-of-scope content).\"]}]\n2a:[\"$\",\"p\",null,{\"children\":\"Let's add the two-levels guardrailing system to the our chatbot:\"}]\n49:T9d7,"])</script><script>self.__next_f.push([1,"from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR_API_KEY\")\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n  message: str\n  \ndef input_guardrail(message: str):\n\n  safe_topics = [\"customer service\", \"product information\", \"order status\", \"returns\", \"shipping\"]\n  prompt = f\"You are a guardrail model that ensures the user's message is appropriate for a customer service chatbot. Only allow messages related to the following topics: \" + \", \".join(safe_topics) + \". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": prompt},\n          {\"role\": \"user\", \"content\": message}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  return reply\n\ndef output_guardrail(message: str):\n\n  unsafe_content = [\"hate speech\", \"personal attacks\", \"sensitive information\", \"inappropriate language\"]\n  prompt = f\"You are a guardrail model that ensures the chatbot's response is appropriate for a customer service context. Messages containing the following types of content are considered inappropriate: \" + \", \".join(unsafe_content) + \". If the message is inappropriate, respond with 'unsafe'. Otherwise, respond with 'safe'.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": prompt},\n          {\"role\": \"user\", \"content\": message}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  return reply\n\n@app.post(\"/chat\")\nasync def chat_endpoint(req: ChatRequest):\n  \n  user_msg = req.message\n  \n  # Input Guardrail Check\n  input_check = input_guardrail(user_msg)\n  if input_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n  \n  # Generate Chatbot Response\n  system_prompt = \"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": user_msg}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n  \n  # Output Guardrail Check\n  output_check = output_guardrail(reply)\n  if output_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n  \n  return {\"reply\": reply}\n"])</script><script>self.__next_f.push([1,"2b:[\"$\",\"$L48\",null,{\"code\":\"$49\"}]\n2c:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of guardrailed chatbot\"}]\n2d:[\"$\",\"p\",null,{\"children\":[\"Boom, our chatbot is now much safer, but its \",[\"$\",\"strong\",null,{\"children\":\"cost has increased dramatically\"}],\". For every user message, we\\nare now making not one, but three separate requests to OpenAI's API: one for the input guardrail, one for chat\\ncompletion, one for the output guardrail.\"]}]\n2e:[\"$\",\"p\",null,{\"children\":[\"The price increase is not even the only issue here: \",[\"$\",\"strong\",null,{\"children\":\"latency has gone up significantly\"}],\" too. Except for cases\\nin which the conversation stops at the input guardrail (if the user query is unsafe, that particular\\nconversation stops there and never makes it to the chat completion or output guardrail), all user messages now go\\nthrough three separate round-trips to the OpenAI servers instead of just one.\"]}]\n"])</script><script>self.__next_f.push([1,"2f:[\"$\",\"p\",null,{\"children\":[\"To put things into perspective, this new implementation consumes, on average, an additional 130 input tokens\\nand 2 output tokens per user message. Assuming the same average number of tokens per user message as before, this\\ntranslates into an overall cost of $0.7 per 1K user messages. Since the cost of the guardrail-free chatbot\\nwas $0.42 per 1K user messages, the guardrailed chatbot is around \",[\"$\",\"strong\",null,{\"children\":\"67% more expensive\"}],\". Things don't\\nlook better latency-wise: the new average measured latency is 3.4 seconds, which means our\\nguardrailed chatbot is \",[\"$\",\"strong\",null,{\"children\":\"2.5x slower\"}],\" than the guardrai-free version.\"]}]\n"])</script><script>self.__next_f.push([1,"30:[\"$\",\"$L1b\",null,{\"src\":\"/images/blog/guardrail-cost-chart.png\",\"alt\":\"Guardrail cost chart\",\"width\":0,\"height\":0,\"style\":{\"width\":\"100%\",\"height\":\"auto\"},\"unoptimized\":true,\"className\":\"mb-5\"}]\n31:[\"$\",\"h2\",null,{\"children\":\"Offloading guardrail tasks to a self-hosted model with Artifex\"}]\n32:[\"$\",\"p\",null,{\"children\":[\"We now come to the main point of this tutorial. If only we had a way to offload guardrail-related\\nqueries to a small guardrail model that runs on the same server as the chatbot backend, instead of\\nsending them to OpenAI's API, we could reduce the number of API calls by two-thirds,\\nthereby \",[\"$\",\"strong\",null,{\"children\":\"cutting costs and latency\"}],\" by a significant amount.\"]}]\n33:[\"$\",\"p\",null,{\"children\":[\"Such a solution is, in fact, possible with \",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"Artifex\"}],\".\\nArtifex is an open-source Python library for using and fine-tuning small, task-specific LLMs without the need for labeled\\ndata or GPUs.\"]}]\n34:[\"$\",\"p\",null,{\"children\":\"We will again create two separate guardrails, one for the input message and one for the output message, then\\nreplace the API-based guardrails with the ones running locally.\"}]\n35:[\"$\",\"p\",null,{\"children\":\"First of all, install the Artifex library with\"}]\n36:[\"$\",\"$L48\",null,{\"code\":\"pip install artifex\"}]\n37:[\"$\",\"p\",null,{\"children\":\"Creating a guardrail model with Artifex is as simple as the following:\"}]\n38:[\"$\",\"$L48\",null,{\"code\":\"from artifex import Artifex\\n\\ngr = Artifex().guardrail\\n\\nmodel_output_path = \\\"./input_guardrail/\\\"\\n\\ngr.train(\\n  instructions=[\\n      \\\"Queries related to customer service, product information, order status, returns and shipping are allowed.\\\",\\n      \\\"Everything else is not allowed.\\\"\\n  ],\\n  output_path=model_output_path\\n)\\n\"}]\n39:[\"$\",\"p\",null,{\"children\":[\"There. All we have done is passing to the \",[\"$\",\"code\",null,{\"children\":\"instructions\"}],\" argument of \",[\"$\",\"code\",null,{\"children\":\"Artifex().guardr"])</script><script>self.__next_f.push([1,"ail.train()\"}],\" the list of\\nallowed and not allowed queries. Read the two strings inside the list passed as the \",[\"$\",\"code\",null,{\"children\":\"instructions\"}],\"\\nargument: they describe the same allowed and unallowed content that we passed to the\\nAPI-based guardrail's system prompt earlier.\"]}]\n3a:[\"$\",\"p\",null,{\"children\":[\"The training process will take a few minutes. Once it's done, the model will\\nbe saved to the \",[\"$\",\"code\",null,{\"children\":\"./input_guardrail/\"}],\" folder, together with the synthetic dataset that was generated on-the-fly\\nduring training, based on the instructions we provided.\"]}]\n3b:[\"$\",\"p\",null,{\"children\":\"Once the input guardrail model is ready, we can create the output guardrail model in a similar way:\"}]\n3c:[\"$\",\"$L48\",null,{\"code\":\"from artifex import Artifex\\n\\ngr = Artifex().guardrail\\n\\nmodel_output_path = \\\"./output_guardrail/\\\"\\n\\ngr.train(\\n  instructions=[\\n      \\\"Responses that contain hate speech, personal attacks, sensitive information or inappropriate language are not allowed.\\\",\\n      \\\"Everything else is allowed.\\\"\\n  ],\\n  output_path=model_output_path\\n)\\n\"}]\n3d:[\"$\",\"p\",null,{\"children\":\"Now that both guardrail models are ready, we can modify our FastAPI chatbot implementation to use\\nthe guardrail models we just created instead of the API-based ones:\"}]\n4a:T51b,"])</script><script>self.__next_f.push([1,"from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom artifex import Artifex\n\nclient = OpenAI(api_key=\"YOUR_API_KEY\")\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n  message: str\n\ninput_guardrail = Artifex().guardrail\noutput_guardrail = Artifex().guardrail\n\ninput_guardrail.load(\"./input_guardrail\")\noutput_guardrail.load(\"./output_guardrail\")\n\n@app.post(\"/chat\")\nasync def chat_endpoint(req: ChatRequest):\n\n  user_msg = req.message\n\n  # Input Guardrail Check\n  input_check = input_guardrail(user_msg)[0].label\n  if input_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n\n  # Generate Chatbot Response\n  system_prompt = \"You are a helpful customer service assistant for an online store. Do your best to help users with their inquiries about products, orders, and services.\"\n\n  response = client.chat.completions.create(\n      model=\"gpt-4.1\",\n      messages=[\n          {\"role\": \"system\", \"content\": system_prompt},\n          {\"role\": \"user\", \"content\": user_msg}\n      ]\n  )\n\n  reply = response.choices[0].message.content\n\n  # Output Guardrail Check\n  output_check = output_guardrail(reply)[0].label\n  if output_check == \"unsafe\":\n      return {\"reply\": \"I'm sorry, but I cannot help you with that.\"}\n\n  return {\"reply\": reply}\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"$L48\",null,{\"code\":\"$4a\"}]\n3f:[\"$\",\"p\",null,{\"children\":[\"Our code is even simpler than before. We just instantiate two guardrail models with\\n\",[\"$\",\"code\",null,{\"children\":\"Artifex().guardrail\"}],\", then load the two fine-tuned guardrails we created earlier with the \",[\"$\",\"code\",null,{\"children\":\"load()\"}],\"\\nmethod. We can then remove the two \",[\"$\",\"code\",null,{\"children\":\"input_guardrail()\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"output_guardrail()\"}],\" functions entirely,\\nreplacing them with calls to our newly loaded input and output guardrails. The rest is exactly\\nthe same as before.\"]}]\n40:[\"$\",\"p\",null,{\"children\":[\"For more information on how to train, load and perform inference with guardrail models with Artifex, check out\\nthe \",[\"$\",\"a\",null,{\"href\":\"https://docs.tanaos.com/artifex/guardrail/train/\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"\\nArtifex documentation\"}],\".\"]}]\n41:[\"$\",\"h3\",null,{\"children\":\"Cost \u0026 latency assessment of chatbot that uses self-hosted guardrails\"}]\n42:[\"$\",\"p\",null,{\"children\":[\"Since all guardrail-related inference is now happening entirely on our CPU, OpenAI will not bill\\nus for it. Our chatbot cost is therefore back to its original level (prior to adding the API-based\\nguardrails), which is $0.42 per 1K user messages, while its average measured latency is 1.6 seconds.\\nThis translates to a \",[\"$\",\"strong\",null,{\"children\":\"43% reduction in costs and a 53% reduction in latency\"}],\" compared to the\\nchatbot that uses API-based guardrails.\"]}]\n43:[\"$\",\"$L1b\",null,{\"src\":\"/images/blog/local-guardrail-cost-chart.png\",\"alt\":\"Guardrail cost chart\",\"width\":0,\"height\":0,\"style\":{\"width\":\"100%\",\"height\":\"auto\"},\"unoptimized\":true,\"className\":\"mb-5\"}]\n44:[\"$\",\"h2\",null,{\"children\":\"Wrapping up\"}]\n45:[\"$\",\"p\",null,{\"children\":[\"In this tutorial, we have seen how to use Artifex to create small, task-specific guardrail\\nmodels that run on the same server as the chatbot backend, thereby \",[\"$\",\"strong\",null,{\"children\":\"cutting chatbot costs by\\n43% and latency by "])</script><script>self.__next_f.push([1,"53%\"}],\", by offloading guardrail-related queries to the self-hosted models.\"]}]\n46:[\"$\",\"p\",null,{\"children\":\"This approach is not limited to guardrail tasks only: any kind of specialized text\\nclassification task that would otherwise require expensive API calls can be offloaded to a small,\\nlocally running model created with Artifex, thereby reducing costs and latency across the board.\"}]\n47:[\"$\",\"p\",null,{\"children\":[\"If you want to learn more about Artifex and how to use it, check out its\\n\",[\"$\",\"a\",null,{\"href\":\"https://github.com/tanaos/artifex\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"GitHub repository\"}],\" and\\n\",[\"$\",\"a\",null,{\"href\":\"$undefined\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"documentation\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"17:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n13:null\n"])</script><script>self.__next_f.push([1,"15:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail. | Tanaos Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"keywords\",\"content\":\"task-specific LLM,offline NLP,text classification,without training data,NLP,Artifex,0.4.0\"}],[\"$\",\"link\",\"2\",{\"rel\":\"canonical\",\"href\":\"undefined/blog/cut-guardrail-costs/\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:title\",\"content\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"undefined/blog/cut-guardrail-costs/\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:image\",\"content\":\"https://tanaos.com/images/blog/cut-guardrail-costs.png\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image:alt\",\"content\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Cut your chatbot cost and latency by 40% by using a small, self-hosted Guardrail.\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://tanaos.com/images/blog/cut-guardrail-costs.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"1a:\"$15:metadata\"\n"])</script></body></html>